{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Final Assignment\n",
    "\n",
    "In this assignment, you will build a relation extraction model for scientific articles based on the [ScienceIE dataset](https://scienceie.github.io/) in a group of up to 3 students. This is the same dataset that was used for Assignment 2, where you had to train a model to extract keyphrases. You are welcome to build on code any team member already wrote for Assignment 2.\n",
    "\n",
    "You will build and train relation extraction models on the ScienceIE dataset. For this, you will also need to do data preprocessing to convert the ScienceIE data into a format suitable for training a relation extraction models. \n",
    "\n",
    "Your mark will depend on:\n",
    "\n",
    "* your **reasoning behind modelling choices** made\n",
    "* the correct **implementations** of your relation extraction models, and\n",
    "* the **performance** of your models on a held-out test set.\n",
    "\n",
    "To develop your model you have access to:\n",
    "\n",
    "* The data in `data/scienceie/`. Remember to un-tar the data.tar.gz file.\n",
    "* Libraries on the [docker image](https://cloud.docker.com/repository/docker/bjerva/stat-nlp-book) which contains everything in [this image](https://github.com/jupyter/docker-stacks/tree/master/scipy-notebook), including scikit-learn, torch 1.2.0 and tensorflow 1.14.0. \n",
    "\n",
    "\n",
    "As with the previous assignment, since we have to run the notebooks of all students, and because writing efficient code is important, your notebook should run in 10 minutes at most, including package loading time, on your machine.\n",
    "Furthermore, you are welcome to provide a saved version of your model with loading code. In this case loading, testing, and evaluation has to be done in 10 minutes. You can use the dev set to check if this is the case, and assume that it will be fine for the held-out test set if so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Setup Instructions\n",
    "It is important that this file is placed in the **correct directory**. It will not run otherwise. The correct directory is\n",
    "\n",
    "    DIRECTORY_OF_YOUR_BOOK/assignments/2019/final_assignment/problem/\n",
    "    \n",
    "where `DIRECTORY_OF_YOUR_BOOK` is a placeholder for the directory you downloaded the book to. After you placed it there, **rename the file** to your UCPH ID (of the form `xxxxxx`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## General Instructions\n",
    "This notebook will be used by you to provide your solution, and by us to both assess your solution and enter your marks. It contains three types of sections:\n",
    "\n",
    "1. **Setup** Sections: these sections set up code and resources for assessment. **Do not edit these**. \n",
    "2. **Assessment** Sections: these sections are used for both evaluating the output of your code, and for markers to enter their marks. **Do not edit these**. \n",
    "3. **Task** Sections: these sections require your solutions. They may contain stub code, and you are expected to edit this code. For free text answers simply edit the markdown field.  \n",
    "\n",
    "Note that you are free to **create additional notebook cells** within a task section. \n",
    "\n",
    "**Do not share** this assignment publicly, by uploading it online, emailing it to friends etc. \n",
    "\n",
    "**Do not** copy code from the Web or from other students, this will count as plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Submission Instructions\n",
    "\n",
    "To submit your solution:\n",
    "\n",
    "* Make sure that your solution is fully contained in this notebook. \n",
    "* **Rename this notebook to your UCPH ID** (of the form \"xxxxxx\"), if you have not already done so.\n",
    "* Download the notebook in Jupyter via *File -> Download as -> Notebook (.ipynb)*.\n",
    "* Upload the notebook to Absalon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## <font color='green'>Setup 1</font>: Load Libraries\n",
    "This cell loads libraries important for evaluation and assessment of your model. **Do not change it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#! SETUP 1\n",
    "import sys, os\n",
    "_snlp_book_dir = \"../../../../\"\n",
    "sys.path.append(_snlp_book_dir) \n",
    "import math\n",
    "from glob import glob\n",
    "from os.path import isfile, join\n",
    "from statnlpbook.vocab import Vocab\n",
    "from statnlpbook.scienceie import calculateMeasures\n",
    "import shutil\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## <font color='blue'>Task 1</font>: Convert dataset between standoff and IOB format\n",
    "\n",
    "We want to work with [the ScienceIE dataset](https://scienceie.github.io) that can be found in the `data/scienceie/` directory of the repository.  This dataset comes with **standoff annotation** for keyphrases and relations between them.  This means that for each document in the dataset, there are two files: a `.txt` file with the raw sentences, and a `.ann` file with the annotated keyphrases.  \n",
    "\n",
    "For example, this is one of the `.txt` files from the training set:\n",
    "\n",
    "```\n",
    "Failure of structural components is a major concern in the nuclear power industry and represents not only a safety issue, but also a hazard to economic performance. Stress corrosion cracking (SCC), and especially intergranular stress corrosion cracking (IGSCC), have proved to be a significant potential cause of failures in the nuclear industry in materials such as Alloy 600 (74% Ni, 16% Cr and 8% Fe) and stainless steels, especially in Pressurised Water Reactors (PWR) [1–5]. Stress corrosion cracking in pressurized water reactors (PWSCC) occurs in Alloy 600 in safety critical components, such as steam generator tubes, heater sleeves, pressurized instrument penetrations and control rod drive mechanisms [2,6,7]. Understanding the mechanisms that control SCC in this alloy will allow for continued extensions of life in current plant as well as safer designs of future nuclear reactors.\n",
    "```\n",
    "\n",
    "And this is the corresponding `.ann` file:\n",
    "\n",
    "```\n",
    "T1\tMaterial 11 32\tstructural components\n",
    "T2\tProcess 0 32\tFailure of structural components\n",
    "T3\tProcess 254 259\tIGSCC\n",
    "T4\tProcess 213 252\tintergranular stress corrosion cracking\n",
    "*\tSynonym-of T4 T3\n",
    "T5\tProcess 165 190\tStress corrosion cracking\n",
    "T6\tProcess 192 195\tSCC\n",
    "*\tSynonym-of T5 T6\n",
    "T7\tMaterial 367 376\tAlloy 600\n",
    "T8\tMaterial 378 402\t74% Ni, 16% Cr and 8% Fe\n",
    "*\tSynonym-of T7 T8\n",
    "T9\tMaterial 408 424\tstainless steels\n",
    "T10\tMaterial 440 466\tPressurised Water Reactors\n",
    "T11\tMaterial 468 471\tPWR\n",
    "T12\tProcess 480 505\tStress corrosion cracking\n",
    "T13\tMaterial 509 535\tpressurized water reactors\n",
    "T14\tMaterial 537 542\tPWSCC\n",
    "*\tSynonym-of T13 T14\n",
    "T15\tMaterial 554 563\tAlloy 600\n",
    "T16\tMaterial 603 624\tsteam generator tubes\n",
    "T17\tMaterial 626 640\theater sleeves\n",
    "T18\tMaterial 642 677\tpressurized instrument penetrations\n",
    "T19\tMaterial 682 710\tcontrol rod drive mechanisms\n",
    "T20\tMaterial 762 765\tSCC\n",
    "T21\tMaterial 774 779\talloy\n",
    "T22\tMaterial 835 840\tplant\n",
    "T23\tTask 852 892\tsafer designs of future nuclear reactors\n",
    "T24\tMaterial 876 892\tnuclear reactors\n",
    "T25\tMaterial 567 593\tsafety critical components\n",
    "R1\tHyponym-of Arg1:T16 Arg2:T25\t\n",
    "R2\tHyponym-of Arg1:T17 Arg2:T25\t\n",
    "R3\tHyponym-of Arg1:T18 Arg2:T25\t\n",
    "R4\tHyponym-of Arg1:T19 Arg2:T25\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Besides keyphrases, which you are already familiar with from Assignment 2, the `.ann` files also contain relation annotations labeled `Hyponym-of` and `Synonym-of`. These are relations between keyphrases. \n",
    "\n",
    "`Synonym-of` is an undirected relation, meaning that if you see a line like this:\n",
    "\n",
    "```*\tSynonym-of T13 T14```\n",
    "\n",
    "The order of keyphrases could be swapped, i.e. the following would also hold:\n",
    "\n",
    "```*\tSynonym-of T14 T13```\n",
    "\n",
    "The evaluation script will thus be agnostic to the order in which the keyphrases between which `Synonym-of` relations hold are ordered.\n",
    "\n",
    "`Hyponym-of`, on the other hand, is a directed relation, meaning that it is order-sensitive, and that the evaluation script will take the order of keyphrases between which `Hyponym-of` relations hold into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.ann` standoff format is **documented in [the brat documentation](http://brat.nlplab.org/standoff.html).**  \n",
    "You may want to convert the format into some internal representation for training models; however, how you do that is up to you, i.e. you do not have to use IOB format like in Assignment 2. \n",
    "\n",
    "**Further Notes**:\n",
    "- At training time, you you will be provided with plain text documents and `.ann` files with keyphrases and relations\n",
    "- At test time, you will be provided with plain text documents and `.ann` files **with keyphrases only**. This is because your task is to predict relations.\n",
    "- The evaluation script is agnostic to the order of relation triples and relation ids, but should preserve the ids of the keyphrases that will be used in the predicted relations. The evaluation scripts requres the entity annotations to be present as well in the prediction file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General modules\n",
    "import pprint\n",
    "from gensim.models import fasttext\n",
    "from gensim.models import KeyedVectors\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "### Our own modules\n",
    "from Extra_files.modules.load_and_save import load_scienceie, save_to_ann, reformat_to_save\n",
    "from Extra_files.modules.Entity_list_identifyer import List_identifyer, Abr_identifyer\n",
    "from Extra_files.modules.confusion_matrix import plot_confusion_matrix\n",
    "from Extra_files.modules.DataPreparation import entityLocator, addPOStoDic, addLemmatoDic, inputPair, WordEmbedder\n",
    "from Extra_files.modules.MasterVocab import MasterVocab\n",
    "from Extra_files.modules.scaling import downscale\n",
    "from Extra_files.models.dummy_model import _sebastians_dummy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dev_data...\n",
      "Number of entities removed due to overlap: 269 out of 1330\n",
      "Number of entities not identified in text: 1 out of 1061\n",
      "Number of relations lost due to overlap: 10 out of 168\n",
      "Removed references.\n",
      "Concatenated 'i.e.' and 'e.g.'.\n",
      "Loading train_data...\n",
      "Number of entities removed due to overlap: 1337 out of 7405\n",
      "Number of entities not identified in text: 28 out of 6068\n",
      "Number of relations lost due to overlap: 50 out of 673\n",
      "Removed references.\n",
      "Concatenated 'i.e.' and 'e.g.'.\n"
     ]
    }
   ],
   "source": [
    "### LOAD THE DATA\n",
    "pp = pprint.PrettyPrinter(indent=1,depth = None, compact = True).pprint\n",
    "print(\"Loading dev_data...\")\n",
    "dev_data = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"dev\"))\n",
    "print(\"Loading train_data...\")\n",
    "train_data = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"train\"))\n",
    "\n",
    "#print(\"Data example:\")\n",
    "#pp(dev_data['S0021999113005846'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /opt/conda/lib/python3.6/site-packages (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/opt/conda/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "/opt/conda/lib/python3.6/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "# GET SPACY HERE\n",
    "if True:\n",
    "    !python -m spacy download en\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en\")\n",
    "    nlp.tokenizer = nlp.tokenizer.tokens_from_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "engbedder = WordEmbedder()\n",
    "engbedder.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Den endelige funktion til at generere data_X og data_Y\n",
    "\n",
    "def dataX_Y_format(data, nlp_language = nlp, maxvocab = 4092, indices = False):\n",
    "    \"\"\"\n",
    "    This takes the raw data from 'load_scienceie' and converts data_X and data_Y for input to tensorflow.\n",
    "    \n",
    "    Input:\n",
    "        data: The raw data output from 'load_scienceie' (a dictionary)\n",
    "        \n",
    "        indices: If False, it returns the names in the features (word, type-of-entity, POS). \n",
    "                 If True, it converts word, type-of-entity and POS to indices, which can be used for embedding.\n",
    "                 \n",
    "    Output:\n",
    "        data_X, data_Y\n",
    "        \n",
    "        data_X is a list of 2d np-array with shape (sentence_length, feature_length)\n",
    "        data_Y is a 1d np-array\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = MasterVocab(max_vocab_size = maxvocab)\n",
    "    print(\"Creating X and Y..\")\n",
    "    if indices:\n",
    "        #vocab_w, vocab_ent, vocab_pos = GenerateVocabs(data, maxvocab, nlp)\n",
    "        vocab.generateVocabularies(data, nlp_language)\n",
    "    \n",
    "    metadata = []\n",
    "    data_X = []\n",
    "    data_Y = []\n",
    "        \n",
    "    for txt, file in data.items():\n",
    "        addPOStoDic(file, nlp_language)\n",
    "        addLemmatoDic(file, nlp_language)\n",
    "        entities = entityLocator(file)\n",
    "        \n",
    "        #Labels\n",
    "        labels = {}\n",
    "        labels['Synonyms']= [(rel[2],rel[3]) for rel in file['relations'] if rel[1] == 'Synonym']\n",
    "        labels['Hyponyms'] = [(rel[2],rel[3]) for rel in file['relations'] if rel[1] == 'Hyponym']\n",
    "        \n",
    "        # create a 'stair' of combinations\n",
    "        \n",
    "        for i in range(len(entities)):\n",
    "            for j in range(i+1, len(entities)):\n",
    "                \n",
    "                #Extrac X and annotation names\n",
    "                ann_names = (entities[i][0],entities[j][0])\n",
    "                ann_names_reverted = (entities[j][0],entities[i][0])\n",
    "                \n",
    "                xdata = inputPair(entities[i], entities[j], file, nlp_language)\n",
    "                \n",
    "                metadata.append( (txt,entities[i][0],entities[j][0]) )\n",
    "                \n",
    "                #Reformat X\n",
    "                if indices:\n",
    "                    xout = vocab.transformX_toIndex(xdata)\n",
    "                    #xout = createX(xdata, vocab_w, vocab_ent, vocab_pos)\n",
    "                    data_X.append(np.array(xout))\n",
    "                else:\n",
    "                    data_X.append(np.array(xdata))\n",
    "                \n",
    "                ### Extract label\n",
    "                if ann_names in labels['Hyponyms']:\n",
    "                    data_Y.append('Hyponym')\n",
    "                elif ann_names_reverted in labels['Hyponyms']:\n",
    "                    data_Y.append('Hyponym_reverted')\n",
    "                elif ann_names in labels['Synonyms'] or ann_names_reverted in labels['Synonyms']:\n",
    "                    data_Y.append('Synonym')\n",
    "                else:\n",
    "                    data_Y.append('NONE')\n",
    "                    \n",
    "\n",
    "    #We need it all in a single object\n",
    "    data = {'metadata': metadata, 'data_X': data_X, 'data_Y': data_Y, 'Vocab': vocab}\n",
    "    \n",
    "    ### Downscaling:\n",
    "    print(\"Downscaling data..\")\n",
    "    if indices:\n",
    "        data = downscale(data, vocab.word_vocab)\n",
    "    else:\n",
    "        data = downscale(data)\n",
    "    \n",
    "    print('The 4 labels after downscaling:')\n",
    "    pp(Counter(data['data_Y']).most_common())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_m_XY = dataX_Y_format(dev_data,indices = True)\n",
    "\n",
    "if False:\n",
    "    print(data_m_XY['metadata'][0])\n",
    "    pp(data_m_XY['data_X'][0])\n",
    "    print(data_m_XY['data_Y'][0])\n",
    "    \n",
    "    print(data_m_XY['Vocab'].transformX_toLabel(data_m_XY['data_X'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## <font color='blue'>Task 1.1</font>: Develop and Train a Relation Extraction Model with Gold Keyphrases\n",
    "\n",
    "In this task, you develop a relation extraction model and apply it to the ScienceIE dataset.\n",
    "As input to it, at test time, you will have the plain input texts as well as `.ann` files containing gold (i.e. correct) keyphrase annotations. The output should be `.ann` files containing relations between those keyphrases (you should include the keyphrase annotations in the output file as well).\n",
    "\n",
    "A test input/output example is given in folders `data/scienceie/test/`,`data/scienceie/test_pred/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no strict requirements for how to design this model. You are expected to use the knowledge you have gathered throughout this course to design and implement this model. \n",
    "\n",
    "You are welcome to re-use existing code you might have written for other assignments as you see fit.\n",
    "\n",
    "You are free to implement your solution in either PyTorch or Tensorflow, but if you are not sure where to start, we recommend looking at the [Keras API](https://keras.io) which is [integrated into Tensorflow 1.14.0](https://www.tensorflow.org/beta/guide/keras/overview?hl=en)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should improve this cell\n",
    "\n",
    "def create_model(train_data, dev_data):\n",
    "    \"\"\"\n",
    "    Return an instance of a relation extraction model defined over the dataset.\n",
    "    Args:\n",
    "        train_data: the training data the relation extraction detection model should be defined over.\n",
    "        dev_data: the development data the relation extraction detection model can be tuned on.\n",
    "    Returns:\n",
    "        a relation extraction model\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def train_model(model, train_data, dev_data):\n",
    "    \"\"\"Train a relation extraction model on the given dataset.\n",
    "    Args:\n",
    "        model: The model to train\n",
    "        data_train: The dataset to train on\n",
    "        dev_data: the development data the relation extraction detection model can be tuned on\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Reformat with Skjøtt's functions\n",
    "    data_m_XY_train = dataX_Y_format(train_data,indices = True)\n",
    "    data_m_XY_dev = dataX_Y_format(dev_data,indices = True)\n",
    "    \n",
    "    ### Extract X and Y\n",
    "    train_X = data_m_XY_train['data_X']\n",
    "    train_Y = data_m_XY_train['data_Y']\n",
    "    dev_X = data_m_XY_dev['data_X']\n",
    "    dev_Y = data_m_XY_dev['data_Y']\n",
    "    \n",
    "    #### TENSORFLOW MODEL (BEGIN) ------------------------------------------\n",
    "    \n",
    "    \n",
    "    #### TENSORFLOW MODEL (END) --------------------------------------------\n",
    "    \n",
    "    return\n",
    "\n",
    "def make_predictions(model, data, return_raw = False):\n",
    "    \"\"\"Makes predictions on a list of instances\n",
    "    Args:\n",
    "        model: The trained model\n",
    "        data: The dataset to evaluate on\n",
    "    Returns:\n",
    "        The model's predictions for the data.\n",
    "        If 'return_raw = True, it returns 'data_dict' (see below), else it returns reformatted data ready for saving.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Reformat and extract 'data_X'\n",
    "    data_m_XY = dataX_Y_format(data,indices = True)\n",
    "    data_X = data_m_XY['data_X']   \n",
    "    \n",
    "    #### TENSORFLOW MODEL (BEGIN) ------------------------------------------\n",
    "    \n",
    "    predictions = data_m_XY['data_Y'] #The perfect model, this works\n",
    "    #predictions = _sebastians_dummy_model(data_m_XY, data)\n",
    "    \n",
    "    \n",
    "    #### TENSORFLOW MODEL (END) --------------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Save in correct format\n",
    "    data_m_XY['data_Y'] = predictions\n",
    "    if return_raw:\n",
    "        return data_m_XY\n",
    "    else:\n",
    "        data = deepcopy(data)       \n",
    "        save_format = reformat_to_save(data_m_XY)\n",
    "        for txt in data:\n",
    "            data[txt]['relations'] = save_format.get(txt,[]) #We might have txt's with no data after downscaling\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities removed due to overlap: 269 out of 1330\n",
      "Number of entities not identified in text: 1 out of 1061\n",
      "Number of relations lost due to overlap: 10 out of 168\n",
      "Removed references.\n",
      "Concatenated 'i.e.' and 'e.g.'.\n",
      "Creating X and Y..\n",
      "Downscaling data..\n",
      "Kept 2330 datapoints out of 12558 after downscaling.\n",
      "The 4 labels after downscaling:\n",
      "[('NONE', 2173), ('Hyponym_reverted', 67), ('Hyponym', 47), ('Synonym', 43)]\n",
      "           precision   recall f1-score  support\n",
      "\n",
      " Hyponym-of     0.99     0.92     0.95      123\n",
      " Synonym-of     1.00     0.96     0.98       45\n",
      "\n",
      "avg / total     0.99     0.93     0.96      168\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Hyponym-of': {'f1-score': 0.9535864978902953,\n",
       "  'precision': 0.9912280701754386,\n",
       "  'recall': 0.9186991869918699,\n",
       "  'support': 123},\n",
       " 'Synonym-of': {'f1-score': 0.9772727272727273,\n",
       "  'precision': 1.0,\n",
       "  'recall': 0.9555555555555556,\n",
       "  'support': 45},\n",
       " 'overall': {'f1-score': 0.9600000000000001,\n",
       "  'precision': 0.9936305732484076,\n",
       "  'recall': 0.9285714285714286,\n",
       "  'support': 168}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## THIS CELL TESTS THAT EVERYTHING WORKS AS EXPECTED #####\n",
    "#Load data\n",
    "dir_dev = join(_snlp_book_dir, \"data\", \"scienceie\", \"dev\")\n",
    "data_dev = load_scienceie(datadir=dir_dev)\n",
    "\n",
    "#Make predictions\n",
    "data_pred = make_predictions(_, data_dev)\n",
    "dir_pred = join(_snlp_book_dir, \"data\", \"scienceie\", \"dev_pred\")\n",
    "\n",
    "#Save and calculate\n",
    "save_to_ann(data_pred, dir_pred)\n",
    "calculateMeasures(dir_dev, dir_pred, \"keys\") # this will only evaluate the correctness of relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Load data\\ndata_dev = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"dev\"))\\n\\n#Make predictions\\ny_pred = make_predictions(_, data_dev, return_raw = True)[\\'data_Y\\']\\ny_true = dataX_Y_format(data_dev)[\\'data_Y\\']\\n\\n#Plot\\nplot_confusion_matrix(y_pred, y_true, n_labels=None,normalize=True)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CREATE CONFUSION MATRIX OF MY PREDICTIONS #####\n",
    "'''\n",
    "#Load data\n",
    "data_dev = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"dev\"))\n",
    "\n",
    "#Make predictions\n",
    "y_pred = make_predictions(_, data_dev, return_raw = True)['data_Y']\n",
    "y_true = dataX_Y_format(data_dev)['data_Y']\n",
    "\n",
    "#Plot\n",
    "plot_confusion_matrix(y_pred, y_true, n_labels=None,normalize=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL! It will evaluate your model on an unseen dataset!\n",
    "shutil.rmtree(join(_snlp_book_dir, \"data\", \"scienceie\", \"test_pred\")) # clean after previous\n",
    "\n",
    "data_train = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"train\"))\n",
    "data_dev = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"dev\"))\n",
    "\n",
    "model = create_model(data_train, data_dev)\n",
    "train_model(model, data_train, data_dev)\n",
    "\n",
    "data_test = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"test\"))\n",
    "data_pred = make_predictions(model, data_test)\n",
    "dir_pred = join(_snlp_book_dir, \"data\", \"scienceie\", \"test_pred\")\n",
    "save_to_ann(data_pred, dir_pred)\n",
    "\n",
    "dir_gold = join(_snlp_book_dir, \"data\", \"scienceie\", \"test_gold\")\n",
    "calculateMeasures(dir_gold, dir_pred, \"keys\") # this will only evaluate the correctness of relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 1.1</font>: Correctness of the implementation (20 pts)\n",
    "\n",
    "We assess if your code implements a correct relation extraction model (10 points):\n",
    "\n",
    "* 0-5 pts: the model does not run correctly or does not constitute a relation extraction model\n",
    "* 5-10 pts: the model correctly implements the requirements\n",
    "\n",
    "Additionally, we will assess how well your model performs on an unseen test set (10 points):\n",
    "\n",
    "* 0-5 pts: performance worse than a simple baseline model\n",
    "* 5-10 pts: performance better than a simple baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 1.2</font>: Describe your Approach\n",
    "\n",
    "Enter a maximum 500 words description of your model developed in Task 1.1, its architecture, and the way you trained and tuned it. Motivate your choices, describing potential benefits and downsides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 1.2</font>: Modelling Choices and Motivation (10 pts)\n",
    "\n",
    "\n",
    "Finally, we assess your modelling design choices and how you motivated them, which you summarised in the above cell (10 points):\n",
    "\n",
    "* 0-5 pts: the model design choices do not show high levels of creativity, e.g. re-using code from the lecture out of the box; and they are not moviated well\n",
    "* 5-10 pts: the model design choices show high levels of creativity, e.g. combining different things learned throughout the course, models inspired by further reading, etc.; and they are motivated well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## <font color='blue'>Task 2</font>: Relation Extraction with Weak Supervision\n",
    "\n",
    "In this task, the goal is to improve the performance of your model developed in Task 2 by obtaining more automatically labelled training data using a weak supervision approach. You are not required to change the relation extraction model architure, i.e. it is fine to re-use the one from Task 1, but instead, the requirements are to implement one or more weak supervision strategies.\n",
    "\n",
    "Some possible weak supervision methods for relation extraction will be introduced in the lecture Week 43 (https://github.com/copenlu/stat-nlp-book/blob/master/chapters/relation_extraction_slides.ipynb); the following blog post also serves as a good introduction to this topic: https://hazyresearch.github.io/snorkel/blog/ws_blog_post.html\n",
    "\n",
    "For this task, you are not confined to the training data provided to you, but you are welcome to obtain additional unlabelled datasets and automatically label them using weak supervision methods. \n",
    "\n",
    "The general setup will otherwise be the same as for Task 1:\n",
    "As input to it, you will have the plain input texts as well as `.ann` files containing gold (i.e. correct) keyphrase annotations. The output should be `.ann` files containing relations between those keyphrases.\n",
    "\n",
    "**Important notes**:\n",
    "- You must provide code for the functions below. \n",
    "- If running them on the full dataset exceeds the 10 minute limit, you are welcome to additionally provide a line of code that (down)loads the already weakly annotated data.\n",
    "- The maximum file size for weakly annotated data may not exceed 1GB.\n",
    "\n",
    "A test input/output example is given in folders `data/scienceie/test/`,`data/scienceie/test_pred/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Hypernyms from UMLS\n",
    "from Extra_files.modules.Weak_supervision import add_UMLS\n",
    "\n",
    "#data_m_XY = add_UMLS(data_m_XY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should improve this cell\n",
    "\n",
    "def create_weak_model(train_data, dev_data, **args):\n",
    "    \"\"\"\n",
    "    Return an instance of a relation extraction model defined over the dataset.\n",
    "    Args:\n",
    "        train_data: the training data the relation extraction detection model should be defined over.\n",
    "        dev_data: the development data the relation extraction detection model can be tuned on.\n",
    "        **args: any additional arguments needed, e.g. additional automatically labelled training data\n",
    "    Returns:\n",
    "        a relation extraction model\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def train_weak_model(model, train_data, dev_data, **args):\n",
    "    \"\"\"Train a relation extraction model on the given dataset.\n",
    "    Args:\n",
    "        model: The model to train\n",
    "        data_train: The dataset to train on\n",
    "        dev_data: the development data the relation extraction detection model can be tuned on\n",
    "        **args: any additional arguments needed, e.g. additional automatically labelled training data\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def make_predictions_weak(model, data):\n",
    "    \"\"\"Makes predictions on a list of instances. Can be the same as function developed in Task 1.\n",
    "    Args:\n",
    "        model: The trained model\n",
    "        data: The dataset to evaluate on\n",
    "    Returns:\n",
    "        The model's predictions for the data.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a model and evaluating it on the development set. \n",
    "# Use this to monitor the performance of your model prior to submitting your assignment.\n",
    "data_train = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"train\"))\n",
    "dir_dev = join(_snlp_book_dir, \"data\", \"scienceie\", \"dev\")\n",
    "data_dev = load_scienceie(datadir=dir_dev)\n",
    "\n",
    "model = create_weak_model(data_train, [data_dev])\n",
    "train_weak_model(model, data_train, data_dev)\n",
    "\n",
    "data_pred = make_predictions_weak(model, data_dev)\n",
    "dir_pred = join(_snlp_book_dir, \"data\", \"scienceie\", \"dev_pred\")\n",
    "save_to_ann(data_pred, dir_pred)\n",
    "\n",
    "calculateMeasures(dir_dev, dir_pred, \"keys\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL! It will evaluate your model on an unseen dataset!\n",
    "shutil.rmtree(join(_snlp_book_dir, \"data\", \"scienceie\", \"test_pred\")) # clean after previous\n",
    "\n",
    "data_train = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"train\"))\n",
    "data_dev = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"dev\"))\n",
    "\n",
    "model = create_weak_model(data_train, data_dev)\n",
    "train_weak_model(model, data_train, data_dev)\n",
    "\n",
    "data_test = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"test\"))\n",
    "data_pred = make_predictions_weak(model, data_test)\n",
    "dir_pred = join(_snlp_book_dir, \"data\", \"scienceie\", \"test_pred\")\n",
    "save_to_ann(data_pred, dir_pred)\n",
    "\n",
    "dir_gold = join(_snlp_book_dir, \"data\", \"scienceie\", \"test_gold\")\n",
    "calculateMeasures(dir_gold, dir_pred, \"keys\") # this will only evaluate the correctness of relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_pred = join(_snlp_book_dir, \"data\", \"scienceie\", \"test_pred\")\n",
    "dir_gold = join(_snlp_book_dir, \"data\", \"scienceie\", \"test_gold\")\n",
    "\n",
    "dir_gold = join(_snlp_book_dir, \"data\", \"scienceie\", \"test_gold\")\n",
    "calculateMeasures(dir_gold, dir_pred, \"keys\") # this will only evaluate the correctness of relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 2.1</font>: Correctness of the implementation (20 pts)\n",
    "\n",
    "We assess if your code implements correct weak supervision methods (10 points):\n",
    "\n",
    "* 0-5 pts: the model does not run correctly or the methods implemented do not constitute weak supervision strategies\n",
    "* 5-10 pts: the model correctly implements the requirements\n",
    "\n",
    "Additionally, we will assess how well your model performs on an unseen test set (10 points):\n",
    "\n",
    "* 0-5 pts: performance worse than a simple baseline model\n",
    "* 5-10 pts: performance better than a simple baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 2.2</font>: Describe your Approach\n",
    "\n",
    "Enter a maximum 500 words description of your weak supervision stategies developed in Task 2.1 and the way you trained and tuned them. Motivate your choices, describing potential benefits and downsides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 2.2</font>: Modelling Choices and Motivation (10 pts)\n",
    "\n",
    "\n",
    "Finally, we assess your modelling design choices and how you motivated them, which you summarised in the above cell (10 points):\n",
    "\n",
    "* 0-5 pts: the model design choices do not show high levels of creativity, e.g. re-using code from the lecture out of the box; and they are not moviated well\n",
    "* 5-10 pts: the model design choices show high levels of creativity, e.g. combining different things learned throughout the course, models inspired by further reading, etc.; and they are motivated well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 3</font>: Comparison of relation extraction models\n",
    "\n",
    "Reflect on the models implemented in Tasks 1 and 2. What worked and didn't work well, and how would you explain this? How and when does the performance differ between the models and why might that be? You are expected to perform a small error analysis on the development set in order to answer these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 3</font>: Assess your explanation (20 pts)\n",
    "\n",
    "We will mark the explanation along the following dimension: \n",
    "\n",
    "* Substance (20pts: well-designed error analysis, correctly explained reasons for performance differences between models)"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
