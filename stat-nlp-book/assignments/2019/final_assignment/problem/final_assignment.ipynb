{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Final Assignment\n",
    "\n",
    "In this assignment, you will build a relation extraction model for scientific articles based on the [ScienceIE dataset](https://scienceie.github.io/) in a group of up to 3 students. This is the same dataset that was used for Assignment 2, where you had to train a model to extract keyphrases. You are welcome to build on code any team member already wrote for Assignment 2.\n",
    "\n",
    "You will build and train relation extraction models on the ScienceIE dataset. For this, you will also need to do data preprocessing to convert the ScienceIE data into a format suitable for training a relation extraction models. \n",
    "\n",
    "Your mark will depend on:\n",
    "\n",
    "* your **reasoning behind modelling choices** made\n",
    "* the correct **implementations** of your relation extraction models, and\n",
    "* the **performance** of your models on a held-out test set.\n",
    "\n",
    "To develop your model you have access to:\n",
    "\n",
    "* The data in `data/scienceie/`. Remember to un-tar the data.tar.gz file.\n",
    "* Libraries on the [docker image](https://cloud.docker.com/repository/docker/bjerva/stat-nlp-book) which contains everything in [this image](https://github.com/jupyter/docker-stacks/tree/master/scipy-notebook), including scikit-learn, torch 1.2.0 and tensorflow 1.14.0. \n",
    "\n",
    "\n",
    "As with the previous assignment, since we have to run the notebooks of all students, and because writing efficient code is important, your notebook should run in 10 minutes at most, including package loading time, on your machine.\n",
    "Furthermore, you are welcome to provide a saved version of your model with loading code. In this case loading, testing, and evaluation has to be done in 10 minutes. You can use the dev set to check if this is the case, and assume that it will be fine for the held-out test set if so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Setup Instructions\n",
    "It is important that this file is placed in the **correct directory**. It will not run otherwise. The correct directory is\n",
    "\n",
    "    DIRECTORY_OF_YOUR_BOOK/assignments/2019/final_assignment/problem/\n",
    "    \n",
    "where `DIRECTORY_OF_YOUR_BOOK` is a placeholder for the directory you downloaded the book to. After you placed it there, **rename the file** to your UCPH ID (of the form `xxxxxx`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## General Instructions\n",
    "This notebook will be used by you to provide your solution, and by us to both assess your solution and enter your marks. It contains three types of sections:\n",
    "\n",
    "1. **Setup** Sections: these sections set up code and resources for assessment. **Do not edit these**. \n",
    "2. **Assessment** Sections: these sections are used for both evaluating the output of your code, and for markers to enter their marks. **Do not edit these**. \n",
    "3. **Task** Sections: these sections require your solutions. They may contain stub code, and you are expected to edit this code. For free text answers simply edit the markdown field.  \n",
    "\n",
    "Note that you are free to **create additional notebook cells** within a task section. \n",
    "\n",
    "**Do not share** this assignment publicly, by uploading it online, emailing it to friends etc. \n",
    "\n",
    "**Do not** copy code from the Web or from other students, this will count as plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Submission Instructions\n",
    "\n",
    "To submit your solution:\n",
    "\n",
    "* Make sure that your solution is fully contained in this notebook. \n",
    "* **Rename this notebook to your UCPH ID** (of the form \"xxxxxx\"), if you have not already done so.\n",
    "* Download the notebook in Jupyter via *File -> Download as -> Notebook (.ipynb)*.\n",
    "* Upload the notebook to Absalon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## <font color='green'>Setup 1</font>: Load Libraries\n",
    "This cell loads libraries important for evaluation and assessment of your model. **Do not change it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#! SETUP 1\n",
    "import sys, os\n",
    "_snlp_book_dir = \"../../../../\"\n",
    "sys.path.append(_snlp_book_dir) \n",
    "import math\n",
    "from glob import glob\n",
    "from os.path import isfile, join\n",
    "from statnlpbook.vocab import Vocab\n",
    "from statnlpbook.scienceie import calculateMeasures\n",
    "import shutil\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## <font color='blue'>Task 1</font>: Convert dataset between standoff and IOB format\n",
    "\n",
    "We want to work with [the ScienceIE dataset](https://scienceie.github.io) that can be found in the `data/scienceie/` directory of the repository.  This dataset comes with **standoff annotation** for keyphrases and relations between them.  This means that for each document in the dataset, there are two files: a `.txt` file with the raw sentences, and a `.ann` file with the annotated keyphrases.  \n",
    "\n",
    "For example, this is one of the `.txt` files from the training set:\n",
    "\n",
    "```\n",
    "Failure of structural components is a major concern in the nuclear power industry and represents not only a safety issue, but also a hazard to economic performance. Stress corrosion cracking (SCC), and especially intergranular stress corrosion cracking (IGSCC), have proved to be a significant potential cause of failures in the nuclear industry in materials such as Alloy 600 (74% Ni, 16% Cr and 8% Fe) and stainless steels, especially in Pressurised Water Reactors (PWR) [1â€“5]. Stress corrosion cracking in pressurized water reactors (PWSCC) occurs in Alloy 600 in safety critical components, such as steam generator tubes, heater sleeves, pressurized instrument penetrations and control rod drive mechanisms [2,6,7]. Understanding the mechanisms that control SCC in this alloy will allow for continued extensions of life in current plant as well as safer designs of future nuclear reactors.\n",
    "```\n",
    "\n",
    "And this is the corresponding `.ann` file:\n",
    "\n",
    "```\n",
    "T1\tMaterial 11 32\tstructural components\n",
    "T2\tProcess 0 32\tFailure of structural components\n",
    "T3\tProcess 254 259\tIGSCC\n",
    "T4\tProcess 213 252\tintergranular stress corrosion cracking\n",
    "*\tSynonym-of T4 T3\n",
    "T5\tProcess 165 190\tStress corrosion cracking\n",
    "T6\tProcess 192 195\tSCC\n",
    "*\tSynonym-of T5 T6\n",
    "T7\tMaterial 367 376\tAlloy 600\n",
    "T8\tMaterial 378 402\t74% Ni, 16% Cr and 8% Fe\n",
    "*\tSynonym-of T7 T8\n",
    "T9\tMaterial 408 424\tstainless steels\n",
    "T10\tMaterial 440 466\tPressurised Water Reactors\n",
    "T11\tMaterial 468 471\tPWR\n",
    "T12\tProcess 480 505\tStress corrosion cracking\n",
    "T13\tMaterial 509 535\tpressurized water reactors\n",
    "T14\tMaterial 537 542\tPWSCC\n",
    "*\tSynonym-of T13 T14\n",
    "T15\tMaterial 554 563\tAlloy 600\n",
    "T16\tMaterial 603 624\tsteam generator tubes\n",
    "T17\tMaterial 626 640\theater sleeves\n",
    "T18\tMaterial 642 677\tpressurized instrument penetrations\n",
    "T19\tMaterial 682 710\tcontrol rod drive mechanisms\n",
    "T20\tMaterial 762 765\tSCC\n",
    "T21\tMaterial 774 779\talloy\n",
    "T22\tMaterial 835 840\tplant\n",
    "T23\tTask 852 892\tsafer designs of future nuclear reactors\n",
    "T24\tMaterial 876 892\tnuclear reactors\n",
    "T25\tMaterial 567 593\tsafety critical components\n",
    "R1\tHyponym-of Arg1:T16 Arg2:T25\t\n",
    "R2\tHyponym-of Arg1:T17 Arg2:T25\t\n",
    "R3\tHyponym-of Arg1:T18 Arg2:T25\t\n",
    "R4\tHyponym-of Arg1:T19 Arg2:T25\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Besides keyphrases, which you are already familiar with from Assignment 2, the `.ann` files also contain relation annotations labeled `Hyponym-of` and `Synonym-of`. These are relations between keyphrases. \n",
    "\n",
    "`Synonym-of` is an undirected relation, meaning that if you see a line like this:\n",
    "\n",
    "```*\tSynonym-of T13 T14```\n",
    "\n",
    "The order of keyphrases could be swapped, i.e. the following would also hold:\n",
    "\n",
    "```*\tSynonym-of T14 T13```\n",
    "\n",
    "The evaluation script will thus be agnostic to the order in which the keyphrases between which `Synonym-of` relations hold are ordered.\n",
    "\n",
    "`Hyponym-of`, on the other hand, is a directed relation, meaning that it is order-sensitive, and that the evaluation script will take the order of keyphrases between which `Hyponym-of` relations hold into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.ann` standoff format is **documented in [the brat documentation](http://brat.nlplab.org/standoff.html).**  \n",
    "You may want to convert the format into some internal representation for training models; however, how you do that is up to you, i.e. you do not have to use IOB format like in Assignment 2. \n",
    "\n",
    "**Further Notes**:\n",
    "- At training time, you you will be provided with plain text documents and `.ann` files with keyphrases and relations\n",
    "- At test time, you will be provided with plain text documents and `.ann` files **with keyphrases only**. This is because your task is to predict relations.\n",
    "- The evaluation script is agnostic to the order of relation triples and relation ids, but should preserve the ids of the keyphrases that will be used in the predicted relations. The evaluation scripts requres the entity annotations to be present as well in the prediction file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS CELL IS MERELY HELPER FUNCTIONS\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def load_txt_str(filename, datadir):\n",
    "    with open(join(datadir,filename), 'r') as f: #open the file\n",
    "        contents = f.readlines()\n",
    "        assert len(contents) == 1\n",
    "    return contents[0]\n",
    "\n",
    "def split_txt_str(text_string):\n",
    "        newline_striped = text_string.rstrip() #Remove newline\n",
    "        split_str = re.split('(\\W)', newline_striped) #Split on everything but words\n",
    "            \n",
    "        #Remove empty string and function application:\n",
    "        for i in range(len(split_str)-1,-1,-1):\n",
    "            if split_str[i] in ['','\\u2061']:\n",
    "                del split_str[i]\n",
    "\n",
    "        #Save the location, so it can be put back.   \n",
    "        assert(len(split_str) >=1)\n",
    "        \n",
    "        str_lengths = np.array([len(token) for token in split_str])\n",
    "        #assert(len(str_lengths) >= 1)\n",
    "\n",
    "        begin_char = np.append(np.array([0]),np.cumsum(str_lengths[:-1]))\n",
    "        end_char = np.cumsum(str_lengths)\n",
    "        locations = [(begin_char[i],end_char[i]) for i in range(len(begin_char))]\n",
    " \n",
    "        for i in range(len(split_str)-1,-1,-1):\n",
    "        #Remove space, non-breaking space, zero-width-space, thin space:\n",
    "            if split_str[i] in [' ','\\xa0','\\u200b','\\u2009']:\n",
    "                del split_str[i]\n",
    "                del locations[i]\n",
    "\n",
    "        return split_str, locations\n",
    "\n",
    "def load_ann_file(filename, datadir):\n",
    "    #open the file\n",
    "    with open(join(datadir,filename), 'r',newline = '\\n') as f:\n",
    "        contents = f.readlines()\n",
    "    return contents\n",
    "\n",
    "def split_and_sort_ann(contents):\n",
    "    \n",
    "    #Only keep text-bound annotation\n",
    "    only_entity = [line for line in contents if line[0] == 'T'] \n",
    "\n",
    "    splittet = [line.rstrip().split('\\t') for line in only_entity]\n",
    "    for line in splittet:\n",
    "        line[1] = line[1].split(' ')\n",
    "        line[1][1] = int(line[1][1])\n",
    "        line[1][2] = int(line[1][2])\n",
    "            \n",
    "    ### Order according to start of label, with highest end comming first\n",
    "    sort_indices = np.argsort(np.array([line[1][1] + 1/(2+line[1][2]) for line in splittet]))\n",
    "    sorted_annotation = [splittet[i] for i in sort_indices]\n",
    "        \n",
    "    ### Remove double occurences or overlapping labels from annotation files\n",
    "    label_number = 1\n",
    "    while True:\n",
    "        if label_number >= len(sorted_annotation):\n",
    "            break\n",
    "\n",
    "        #If next label starts before the last ends\n",
    "        if sorted_annotation[label_number-1][1][2] > sorted_annotation[label_number][1][1]:\n",
    "            del sorted_annotation[label_number]\n",
    "        else:\n",
    "            label_number +=1\n",
    "\n",
    "    \n",
    "    return sorted_annotation\n",
    "\n",
    "def ann_to_entities(annotations):\n",
    "    \n",
    "    for line in annotations:\n",
    "        line[2],_ = split_txt_str(line[2])\n",
    "    \n",
    "    entity_types = [line[1][0] for line in annotations]\n",
    "    entity_words = [line[2] for line in annotations]\n",
    "    ann_names = [line[0] for line in annotations]\n",
    "    entity_locations = [(int(line[1][1]),int(line[1][2])) for line in annotations]\n",
    "    \n",
    "    return entity_types, entity_words, ann_names, entity_locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell has the relevant functions 'load_scienceie' and 'save_to_ann'\n",
    "\n",
    "from os import listdir\n",
    "import re\n",
    "\n",
    "def load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"train\")):\n",
    "    \"\"\"\n",
    "    Load the ScienceIE dataset from a given directory and return it in IOB format.\n",
    "    Args:\n",
    "        datadir: The directory to read from, e.g. data/scienceie/train or data/scienceie/dev\n",
    "    Returns:\n",
    "        A dictonary with the example format\n",
    "        \n",
    "        data['file_name'] = {'data': [tokens,IOBtags], #where tokens and IOBtags are lists with same length\n",
    "                             'locations':locations, #A list of tuples with start-position and end-position of every token\n",
    "                             'annotation_names': ann_names #A list of names ['T2','T1',...]\n",
    "                             }\n",
    "    \"\"\"\n",
    "\n",
    "    txt_files  = [f for f in listdir(datadir) if f[-3:] == 'txt']\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        ann_files = [f[:-3] + 'ann' for f in txt_files]\n",
    "        ann_file_exists = True\n",
    "    except:\n",
    "        print('Cannot find annotation files. The returned data set cannot be used for training.')\n",
    "        ann_file_exists = False\n",
    "    \n",
    "    #ann_file_exists = False\n",
    "    \n",
    "    data = {}\n",
    "    counter = 0\n",
    "    for i in range(len(txt_files)):\n",
    "        ann_line = 0\n",
    "        ann_word = 0\n",
    "\n",
    "        org_text = load_txt_str(txt_files[i],datadir)\n",
    "        tokens, locations = split_txt_str(org_text)\n",
    "        \n",
    "        ### This creates the IOBtags (labels) for the training\n",
    "        IOBtags = []\n",
    "        relations = []\n",
    "        my_ann_names = []\n",
    "        \n",
    "        if ann_file_exists:\n",
    "            ann_content = load_ann_file(ann_files[i],datadir)\n",
    "            sorted_annotation = split_and_sort_ann(ann_content)\n",
    "            entity_types, entity_words, ann_names, _ = ann_to_entities(sorted_annotation)\n",
    "            \n",
    "            \n",
    "            for word in tokens:\n",
    "\n",
    "                #If we have been through all annotation-lines\n",
    "                if ann_line >= len(entity_words):\n",
    "                    IOBtags.append('O')\n",
    "                    continue #Don't change ann_line /word \n",
    "\n",
    "                #There is an error in one of the dev .ann files\n",
    "                tmp = entity_words[ann_line][ann_word]\n",
    "                if tmp == 'echniques':\n",
    "                    tmp = 'techniques'\n",
    "\n",
    "                #If this word is not the next we have in annotation\n",
    "                if word != tmp:\n",
    "                    IOBtags.append('O')\n",
    "                    continue #Don't change ann_line /word\n",
    "\n",
    "                #If this word is the next in annotation\n",
    "                if ann_word == 0:\n",
    "                    prefix = 'B'\n",
    "                    my_ann_names.append(ann_names[ann_line])\n",
    "                else:\n",
    "                    prefix ='I'\n",
    "                \n",
    "                suffix = entity_types[ann_line]\n",
    "                IOBtags.append(prefix + '-' + suffix)\n",
    "\n",
    "                #Increase annotation word and line\n",
    "                if ann_word < len(entity_words[ann_line])-1:\n",
    "                    ann_word += 1\n",
    "                else:\n",
    "                    ann_line +=1\n",
    "                    ann_word = 0\n",
    "                #end-if\n",
    "            #end-for tokens\n",
    "            \n",
    "            #extract synonym/hyponyms and add them to data\n",
    "            # get */R\n",
    "            non_entity = [line for line in ann_content if line[0] != 'T']\n",
    "            non_entity_split = [re.split(r'[\\- :\\t]',line.rstrip()) for line in non_entity]\n",
    "            \n",
    "            remove = ['','of','Arg1','Arg2']\n",
    "            relations = [tuple(token for token in line if token not in remove) for line in non_entity_split]\n",
    "            \n",
    "            ### Only relations where the entities are in fact found (survived the overlap cleaning)\n",
    "            rels_found = [rel for rel in relations if \\\n",
    "                          rel[2] in my_ann_names and \\\n",
    "                          rel[3] in my_ann_names]\n",
    "        \n",
    "        ### Check if we found all relations\n",
    "        if len(rels_found) != len(relations):\n",
    "            counter += 1\n",
    "            \n",
    "        #Add to output dict\n",
    "        data[txt_files[i][:-4]] = {'tokens': tokens,\n",
    "                                   'IOBtags': IOBtags,\n",
    "                                   'locations':locations,\n",
    "                                   'annotation_names': my_ann_names,\n",
    "                                   'relations': relations}\n",
    "    \n",
    "    print(\"Number of files, with only subset found: {}\".format(counter))\n",
    "    print(\"Total number of files: {}\".format(len(data)))\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_to_ann(data, datadfir):\n",
    "    \"\"\"\n",
    "    Save annotations in IOB format back to .ann files.\n",
    "    Args:\n",
    "        data: The annotations in IOB format\n",
    "        datadir: The directory to save to, e.g. data/scienceie/predictions\n",
    "    \"\"\"\n",
    "    file_names = list(data.keys())\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        file = data[file_name]\n",
    "        \n",
    "        #If two 'B-...' arrives, begin then new, but if 'I-...' after 'O', let it be 'B-...'\n",
    "        \n",
    "        tokens = np.array(file['tokens'])\n",
    "        IOBtags = np.array(file['IOBtags'])\n",
    "        locations = np.array(file['locations'])\n",
    "\n",
    "        #Put into one\n",
    "        ann_tags = []\n",
    "        ann_names = file['annotation_names'] #Maybe None\n",
    "        ann_locations = []\n",
    "        ann_entities = []\n",
    "\n",
    "        #IOBtag (str), locations (tuple), entity (list with start stop at every)\n",
    "        \n",
    "        tmp = []\n",
    "        last_tag = 'O'\n",
    "        \n",
    "        for index in range(len(tokens)):\n",
    "            \n",
    "            tag = IOBtags[index]\n",
    "            word = tokens[index]\n",
    "            location = tuple(locations[index])\n",
    "            \n",
    "            if tag == 'O':\n",
    "                pass\n",
    "            elif tag[0] == 'B':\n",
    "                ann_tags.append(tag[2:])\n",
    "                tmp.append([[word,location]])\n",
    "            elif tag[0] == 'I' and last_tag == 'O': #Begin new_line\n",
    "                ann_tags.append(tag[2:])\n",
    "                tmp.append([[word,location]])\n",
    "            elif tag[0] == 'I' and last_tag[2:] != tag[2:]: #Begin new_line\n",
    "                ann_tags.append(tag[2:])\n",
    "                tmp.append([[word,location]])\n",
    "            elif tag[0] == 'I' and last_tag[2:] == tag[2:]: #Continue this line\n",
    "                tmp[-1].append([word,location])\n",
    "            else:\n",
    "                print('Tag: ',tag)\n",
    "                print('Last_tag: ',last_tag)\n",
    "                raise Exception(\"This should not happen\")\n",
    "                \n",
    "            last_tag = tag\n",
    "\n",
    "\n",
    "        sentences = []\n",
    "        for sentence in tmp:\n",
    "            ann_locations.append((sentence[0][1][0],sentence[-1][1][1]))\n",
    "\n",
    "            entity_str = ''\n",
    "\n",
    "            n=0\n",
    "\n",
    "            while True:\n",
    "                entity_str += sentence[n][0]\n",
    "                n +=1\n",
    "\n",
    "                if len(sentence) <= n:\n",
    "                    break\n",
    "\n",
    "                n_spaces = sentence[n][1][0] - sentence[n-1][1][1]\n",
    "                \n",
    "                try:\n",
    "                    assert (n_spaces in [0,1])\n",
    "                except:\n",
    "                    n_spaces = 1\n",
    "                    \n",
    "                for i in range(n_spaces):\n",
    "                    entity_str += ' '\n",
    "\n",
    "            sentences.append(entity_str)\n",
    "\n",
    "        if ann_names is None:\n",
    "            ann_names = ['T' + str(i) for i in range(1,len(sentenses)+1)]\n",
    "        \n",
    "        ann_sorted = [[ann_names[i],\n",
    "                       [ann_tags[i],ann_locations[i][0],ann_locations[i][1]],\n",
    "                       sentences[i]] for i in range(len(ann_names))]\n",
    "        \n",
    "        #Re-sort it back. (Only makes difference if we have ann_names)\n",
    "        sort_indices = np.argsort(np.array([int(line[0][1:]) for line in ann_sorted]))\n",
    "        org_sorting = [ann_sorted[i] for i in sort_indices]\n",
    "        \n",
    "        #Concatenate \n",
    "        for line in org_sorting:\n",
    "            line[1] = line[1][0] + ' ' + str(line[1][1]) + ' ' + str(line[1][2])\n",
    "\n",
    "        #Save the bloody thing\n",
    "        prediction_dir = datadfir\n",
    "        if not os.path.exists(prediction_dir):\n",
    "            os.makedirs(prediction_dir)\n",
    "\n",
    "        fname = join(datadfir,file_name + '.ann')\n",
    "        with open(fname,'w') as file:\n",
    "            for line in org_sorting:\n",
    "                tmp = '\\t'.join(line)\n",
    "                tmp += '\\n'\n",
    "                file.write(tmp)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files, with only subset found: 5\n",
      "Total number of files: 50\n"
     ]
    }
   ],
   "source": [
    "### VISER LIGE HVORDAN OUTPUT SER UD\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=1,depth = None, compact = True).pprint\n",
    "\n",
    "dev_data = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"dev\"))\n",
    "#the full file\n",
    "#pp(dev_data['S0021999113005846'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files, with only subset found: 37\n",
      "Total number of files: 350\n"
     ]
    }
   ],
   "source": [
    "train_data = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are interested in WORD, RELDIST1, RELDIST2, IOB, POS\n",
    "# Combine the name of T* and the IOBs\n",
    "indata = dev_data['S0010938X13003818']\n",
    "\n",
    "def entityLocator(indata):\n",
    "    # line for each T: T, start, end, entity\n",
    "    entities = []\n",
    "    name_counter = 0\n",
    "    i = 0\n",
    "    flag = 0\n",
    "    for tag in indata['IOBtags']:\n",
    "        if tag[0] == 'B':\n",
    "            name = indata['annotation_names'][name_counter]\n",
    "            tg = tag\n",
    "            start = i\n",
    "            name_counter = name_counter + 1\n",
    "            flag = 1 # flag if we are currently inside iob\n",
    "        if tag[0] == 'O' and flag == 1:\n",
    "            # if entity ended - submit to entities\n",
    "            end = i-1\n",
    "            entities.append((name, tg, start, end))\n",
    "            flag = 0\n",
    "        #end-if\n",
    "        i = i + 1\n",
    "    #end-for\n",
    "    if flag == 1:\n",
    "        #submit\n",
    "        entities.append((name, tg, start, i-1))\n",
    "        flag = 0\n",
    "    #end-tricks\n",
    "    return entities\n",
    "#end-def\n",
    "if False:\n",
    "    indata = dev_data['S0010938X13003818']\n",
    "    print(entityLocator(indata))\n",
    "#end-if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Part-of-speech (spacy) - Stanford coreNLP is a java program\n",
    "\n",
    "## Downloads English SpaCy models and performs part-of-speech tagging on our dataset.\n",
    "## You do not need to modify anything here.\n",
    "if False:\n",
    "    !python -m spacy download en\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en\")\n",
    "    nlp.tokenizer = nlp.tokenizer.tokens_from_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.lang.en.English'>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "### To load objects\n",
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# To get scispacy to work, you need to first install scispacy. Restart kernel, set installer to False\n",
    "# then run the pip install of the model and load. Otherwise it will not work for some reason.\n",
    "if False:\n",
    "    #https://github.com/allenai/scispacy\n",
    "    import numpy\n",
    "    #!pip install pybind11\n",
    "    #!pip install scispacy --no-deps\n",
    "    !pip install scispacy\n",
    "    \n",
    "if False:\n",
    "    import scispacy\n",
    "    #!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n",
    "    import spacy\n",
    "    !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz\n",
    "    #!python -m spacy download en_core_sci_sm\n",
    "    #import en_core_sci_sm\n",
    "    #nlp = spacy.load(\"en_core_sci_lg\")\n",
    "    nlp = spacy.load(\"en_core_sci_sm\")\n",
    "    \n",
    "    # lets pickle this kid\n",
    "    #save_obj(nlp, \"scispacy\")\n",
    "    nlp.to_disk(\"scispacy\")\n",
    "    # build package\n",
    "    !python -m spacy package scispacy scispacyout\n",
    "    \n",
    "    del nlp\n",
    "    print(\"new POS/Lemma model saved.\")\n",
    "    \n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"scispacyout/en_core_sci_sm-0.2.4/en_core_sci_sm/en_core_sci_sm-0.2.4\")\n",
    "#nlp = spacy.blank(\"en\").from_disk(\"scispacy\")\n",
    "nlp.tokenizer = nlp.tokenizer.tokens_from_list\n",
    "print(type(nlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stanford coreNLP is a java program but can be accessed via python:\n",
    "# https://www.khalidalnajjar.com/setup-use-stanford-corenlp-server-python/\n",
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'VERB', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'AUX', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'CCONJ', 'ADP', 'DET', 'VERB', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT', 'ADP', 'NOUN', 'PART', 'VERB', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'NUM', 'AUX', 'VERB', 'PUNCT', 'ADV', 'PUNCT', 'DET', 'NOUN', 'ADP', 'ADV', 'ADJ', 'NOUN', 'AUX', 'ADV', 'VERB', 'ADP', 'DET', 'NUM', 'PUNCT', 'NUM', 'NOUN', 'CCONJ', 'PRON', 'NOUN', 'ADP', 'NOUN', 'PART', 'DET', 'ADJ', 'NOUN', 'NOUN', 'AUX', 'VERB', 'ADP', 'VERB', 'DET', 'NOUN', 'PUNCT', 'PROPN', 'NUM', 'NOUN', 'PUNCT', 'PART', 'PUNCT', 'ADJ', 'NOUN', 'PUNCT', 'DET', 'ADJ', 'NOUN', 'NOUN', 'AUX', 'VERB', 'VERB', 'ADP', 'PUNCT', 'NOUN', 'VERB', 'NOUN', 'PUNCT', 'VERB', 'ADP', 'NUM', 'NOUN', 'ADP', 'NUM', 'NOUN', 'ADP', 'DET', 'NOUN', 'PUNCT', 'ADP', 'NOUN', 'PUNCT', 'DET', 'NUM', 'NOUN', 'VERB', 'AUX', 'VERB', 'PART', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'ADV', 'VERB', 'ADP', 'DET', 'NOUN', 'VERB', 'ADP', 'ADJ', 'NOUN', 'PUNCT', 'DET', 'ADJ', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'VERB', 'NUM', 'NOUN', 'NUM', 'PUNCT', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'ADP', 'DET', 'NOUN', 'SCONJ', 'DET', 'NOUN', 'ADJ', 'ADP', 'NOUN', 'AUX', 'ADJ', 'VERB', 'ADP', 'DET', 'NOUN', 'ADJ', 'ADP', 'NUM', 'PUNCT', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'VERB', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NUM', 'PUNCT', 'NUM', 'CCONJ', 'NUM', 'CCONJ', 'VERB', 'DET', 'NOUN', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "# Get the POS from either Spacy/Stanford-coreNLP\n",
    "\n",
    "#train_pos = [[token.pos_ for token in sent] for sent in nlp.pipe(train_tokens)]\n",
    "#dev_pos = [[token.pos_ for token in sent] for sent in nlp.pipe(dev_tokens)]\n",
    "def getPOS(data_dic, mode = 'spacy'):\n",
    "    if mode == 'spacy':\n",
    "        return [[token.pos_ for token in sent] for sent in nlp.pipe([data_dic['tokens']])][0]\n",
    "    else:\n",
    "        raise ValueError('The mode: \\'%s\\' is not implemented' % (mode))\n",
    "    #end-if\n",
    "#end-def\n",
    "\n",
    "# append POS to data if they do not exist yet\n",
    "#if not 'POS' in indata.keys():\n",
    "\n",
    "def addPOStoDic(data_dic):\n",
    "    pos = getPOS(data_dic) # do not override default!\n",
    "    # assert lenght are equal - otherwise shit has hit the fan!\n",
    "    assert len(pos) == len(data_dic['tokens']), 'POS length does not match token lengths!'\n",
    "    data_dic['POS'] = pos\n",
    "    pass\n",
    "\n",
    "if True:\n",
    "    indata = dev_data['S0010938X13003818']\n",
    "    addPOStoDic(indata)\n",
    "    print(indata['POS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Based', 'on', 'the', 'theoretical', 'analysis', ',', 'the', 'value', 'of', 'the', 'measuring', 'resistor', ',', 'Rm', ',', 'has', 'no', 'effect', 'on', 'the', 'corrosion', 'process', 'and', 'on', 'the', 'estimated', 'value', 'of', 'noise', 'resistance', '.', 'In', 'order', 'to', 'validate', 'this', 'conclusion', ',', 'the', 'experiment', 'of', 'Fig', '.', '9', 'was', 'performed', '.', 'Specifically', ',', 'a', 'pair', 'of', 'nominally', 'identical', 'specimens', 'was', 'initially', 'coupled', 'by', 'a', '4', '.', '7kÎ©', 'resistor', 'and', 'their', 'potential', 'with', 'respect', 'to', 'a', 'saturated', 'calomel', 'electrode', 'was', 'recorded', 'by', 'using', 'a', 'NI', '-', 'USB', '6009', 'analog', '-', 'to', '-', 'digital', 'converter', '.', 'The', 'electrochemical', 'noise', 'signal', 'was', 'recorded', 'using', 'in', '-', 'house', 'developed', 'software', ',', 'acquiring', 'at', '1023Hz', 'segments', 'of', '1000', 'points', 'at', 'each', 'iteration', '.', 'Between', 'iterations', ',', 'the', '1000', 'values', 'acquired', 'were', 'averaged', 'to', 'obtain', 'a', 'single', 'value', 'of', 'potential', ',', 'subsequently', 'saved', 'to', 'the', 'file', 'used', 'for', 'later', 'processing', '.', 'The', 'final', 'dataset', 'comprised', 'potential', 'values', 'spaced', '1', 'Â±', '0', '.', '05s', 'in', 'time', '.', 'Under', 'the', 'assumption', 'that', 'the', 'noise', 'present', 'above', '1023Hz', 'is', 'negligible', 'compared', 'with', 'the', 'noise', 'present', 'below', '0', '.', '5Hz', ',', 'this', 'procedure', 'enables', 'an', 'accurate', 'recording', 'of', 'the', 'potential', 'noise', 'in', 'the', 'frequencies', 'of', 'interest', ',', 'avoiding', 'aliasing', 'of', 'frequencies', 'between', '0', '.', '5', 'and', '1023Hz', 'and', 'minimizing', 'the', '50Hz', 'interference', 'from', 'the', 'mains', 'supply', '.']\n",
      "['base', 'on', 'the', 'theoretical', 'analysis', ',', 'the', 'value', 'of', 'the', 'measure', 'resistor', ',', 'rm', ',', 'have', 'no', 'effect', 'on', 'the', 'corrosion', 'process', 'and', 'on', 'the', 'estimate', 'value', 'of', 'noise', 'resistance', '.', 'in', 'order', 'to', 'validate', 'this', 'conclusion', ',', 'the', 'experiment', 'of', 'Fig', '.', '9', 'be', 'perform', '.', 'specifically', ',', 'a', 'pair', 'of', 'nominally', 'identical', 'specimen', 'be', 'initially', 'couple', 'by', 'a', '4', '.', '7kÏ‰', 'resistor', 'and', '-PRON-', 'potential', 'with', 'respect', 'to', 'a', 'saturated', 'calomel', 'electrode', 'be', 'record', 'by', 'use', 'a', 'ni', '-', 'USB', '6009', 'analog', '-', 'to', '-', 'digital', 'converter', '.', 'the', 'electrochemical', 'noise', 'signal', 'be', 'record', 'use', 'in', '-', 'house', 'develop', 'software', ',', 'acquire', 'at', '1023hz', 'segment', 'of', '1000', 'point', 'at', 'each', 'iteration', '.', 'between', 'iteration', ',', 'the', '1000', 'value', 'acquire', 'be', 'average', 'to', 'obtain', 'a', 'single', 'value', 'of', 'potential', ',', 'subsequently', 'save', 'to', 'the', 'file', 'use', 'for', 'later', 'processing', '.', 'the', 'final', 'dataset', 'comprise', 'potential', 'value', 'space', '1', 'Â±', '0', '.', '05s', 'in', 'time', '.', 'under', 'the', 'assumption', 'that', 'the', 'noise', 'present', 'above', '1023hz', 'be', 'negligible', 'compare', 'with', 'the', 'noise', 'present', 'below', '0', '.', '5hz', ',', 'this', 'procedure', 'enable', 'an', 'accurate', 'recording', 'of', 'the', 'potential', 'noise', 'in', 'the', 'frequency', 'of', 'interest', ',', 'avoid', 'aliasing', 'of', 'frequency', 'between', '0', '.', '5', 'and', '1023hz', 'and', 'minimize', 'the', '50hz', 'interference', 'from', 'the', 'main', 'supply', '.']\n"
     ]
    }
   ],
   "source": [
    "# Get the POS from either Spacy/Stanford-coreNLP\n",
    "\n",
    "#train_pos = [[token.pos_ for token in sent] for sent in nlp.pipe(train_tokens)]\n",
    "#dev_pos = [[token.pos_ for token in sent] for sent in nlp.pipe(dev_tokens)]\n",
    "def getLemma(data_dic, mode = 'spacy'):\n",
    "    if mode == 'spacy':\n",
    "        return [[token.lemma_ for token in sent] for sent in nlp.pipe([data_dic['tokens']])][0]\n",
    "    else:\n",
    "        raise ValueError('The mode: \\'%s\\' is not implemented' % (mode))\n",
    "    #end-if\n",
    "#end-def\n",
    "\n",
    "# append POS to data if they do not exist yet\n",
    "#if not 'POS' in indata.keys():\n",
    "\n",
    "def addLemmatoDic(data_dic):\n",
    "    lemma = getLemma(data_dic) # do not override default!\n",
    "    # assert lenght are equal - otherwise shit has hit the fan!\n",
    "    assert len(lemma) == len(data_dic['tokens']), 'Lemma length does not match token lengths!'\n",
    "    data_dic['Lemma'] = lemma\n",
    "    pass\n",
    "\n",
    "if True:\n",
    "    indata = dev_data['S0010938X13003818']\n",
    "    addPOStoDic(indata)\n",
    "    addLemmatoDic(indata)\n",
    "    print(indata['tokens'])\n",
    "    print(indata['Lemma'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create input pair given entities from entitylocator and data\n",
    "\n",
    "def inputPair(entityA, entityB, indata):\n",
    "    out = []\n",
    "    # get distances\n",
    "    _, _, A_start, A_end = entityA\n",
    "    _, _, B_start, B_end = entityB\n",
    "    \n",
    "    # check if POS\n",
    "    if not 'POS' in indata.keys():\n",
    "        # warn user\n",
    "        print('No POS in data dictionary - adding them...')\n",
    "        # add them\n",
    "        addPOStoDic(indata)\n",
    "        print('POS added to data dictionary.')\n",
    "    \n",
    "    # Get tokens inbetween\n",
    "    i = 0\n",
    "    for token in indata['tokens']:\n",
    "        # A < B\n",
    "        if i in range(A_start, B_end+1):\n",
    "            relA = max(i-A_end, 0)\n",
    "            relB = min(i-B_start,0)\n",
    "            # get POS\n",
    "            out.append([token, relA, relB, indata['IOBtags'][i], indata['POS'][i]])\n",
    "        # B < A\n",
    "        if i in range(B_start, A_end+1):\n",
    "            relA = max(A_start-i, 0)\n",
    "            relB = min(B_end-i,0)\n",
    "            # get POS\n",
    "            out.append([token, relA, relB, indata['IOBtags'][i], indata['POS'][i]])\n",
    "        #end-if\n",
    "        i = i+1\n",
    "    #end-for\n",
    "    return out\n",
    "if False:\n",
    "    indata = dev_data['S0010938X13003818']\n",
    "    entities = entityLocator(indata)\n",
    "    A = entities[0]\n",
    "    B = entities[1]\n",
    "    print(A); print(B)\n",
    "    # works both ways\n",
    "    print(inputPair(A, B, indata))\n",
    "    print(inputPair(B, A, indata))\n",
    "#end-if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP PRE-TRAINED-WORD-EMBEDDINGS\n",
    "from gensim.models import fasttext\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Create model\n",
    "if False:\n",
    "    engmodel = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec', limit=3000)\n",
    "    engmodel.save(\"engmodel.model\")\n",
    "    del engmodel\n",
    "\n",
    "class WordEmbedder:\n",
    "    def __init__(self, vmodel):\n",
    "        self.vmodel = vmodel\n",
    "        self.length = len(vmodel.vectors[0])\n",
    "        \n",
    "    def getEmbedding(self, word):\n",
    "        if word in self.vmodel.vocab:\n",
    "            return self.vmodel[word]\n",
    "        else:\n",
    "            return np.zeros(self.length)\n",
    "#end-class\n",
    "\n",
    "# Get model\n",
    "engmodel = KeyedVectors.load(\"engmodel.model\")\n",
    "engbedder = WordEmbedder(engmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def GenerateVocabs(fulldata, maxdistance, maxsize):\n",
    "    fulldata = dev_data\n",
    "    maxlen = maxdistance\n",
    "\n",
    "    fulldata = copy.deepcopy(fulldata)\n",
    "    # Create POS for all before we create vocab\n",
    "    for entry in list(fulldata.values()):\n",
    "        addPOStoDic(entry)\n",
    "    # we introduce four vocabs: word, dist, entity, pos\n",
    "    vocab_w = Vocab.from_iterable([fulldata[i]['tokens'] for i in fulldata], max_size = maxsize, )\n",
    "    vocab_dist = Vocab.from_iterable(range(-maxlen, maxlen))\n",
    "    vocab_ent = Vocab.from_iterable([fulldata[i]['IOBtags'] for i in fulldata]) # these should have all\n",
    "    vocab_pos = Vocab.from_iterable([fulldata[i]['POS'] for i in fulldata]) # and so should these!\n",
    "    return vocab_w, vocab_dist, vocab_ent, vocab_pos\n",
    "if True:\n",
    "    vocab_w, vocab_dist, vocab_ent, vocab_pos = GenerateVocabs(dev_data, maxdistance = 350, maxsize = 4092)\n",
    "    pos_in_vocab = [vocab_pos.get_label(i) for i in range(len(vocab_pos))]\n",
    "    print(pos_in_vocab)\n",
    "    print(len(pos_in_vocab)) # len = 18 => 18 tags + <pad> equivalent to all POS except <space>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input from strings to ints for embedding layer (or use pre-trained embeddings):\n",
    "def createX(xdata, vocab_words, vocab_distances, vocab_entities, vocab_pos):\n",
    "    out = [[vocab_words.map_to_index([w[0]])[0],                # token\n",
    "            vocab_distances.map_to_index([w[1]])[0],            # dist 1\n",
    "            vocab_distances.map_to_index([w[2]])[0],            # dist 2\n",
    "             vocab_entities.map_to_index([w[3]])[0],            # entities\n",
    "             vocab_pos.map_to_index([w[4]])[0]] for w in xdata] # pos\n",
    "    return out\n",
    "\n",
    "def createXEmbeddings(xdata, embedder, vocab_distances, vocab_entities, vocab_pos):\n",
    "    out = [[embedder.getEmbedding(w[0]),\n",
    "            vocab_distances.map_to_index([w[1]])[0],\n",
    "            vocab_distances.map_to_index([w[2]])[0],\n",
    "             vocab_entities.map_to_index([w[3]])[0],\n",
    "             vocab_pos.map_to_index([w[4]])[0]] for w in xdata]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Den endelige funktion til at generere data_X og data_Y\n",
    "from collections import Counter\n",
    "\n",
    "def dataX_Y_format(data, indices = False):\n",
    "    \"\"\"\n",
    "    This takes the raw data from 'load_scienceie' and converts data_X and data_Y for input to tensorflow.\n",
    "    \n",
    "    Input:\n",
    "        data: The raw data output from 'load_scienceie' (a dictionary)\n",
    "        \n",
    "        indices: If False, it returns the names in the features (word, type-of-entity, POS). \n",
    "                 If True, it converts word, type-of-entity and POS to indices, which can be used for embedding.\n",
    "                 \n",
    "    Output:\n",
    "        data_X, data_Y\n",
    "        \n",
    "        data_X is a list of 2d np-array with shape (sentence_length, feature_length)\n",
    "        data_Y is a 1d np-array\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if indices:\n",
    "        vocab_w, vocab_dist, vocab_ent, vocab_pos = GenerateVocabs(dev_data, 350, 4092)\n",
    "    \n",
    "    files = dev_data.values()\n",
    "    \n",
    "    data_case = []\n",
    "    data_X = []\n",
    "    data_Y = []\n",
    "        \n",
    "    for file in files:\n",
    "        addPOStoDic(file)\n",
    "        entities = entityLocator(file)\n",
    "        \n",
    "        #Labels\n",
    "        labels = {}\n",
    "        labels['synonyms']= [(rel[2],rel[3]) for rel in file['relations'] if rel[1] == 'Synonym']\n",
    "        labels['hyponyms'] = [(rel[2],rel[3]) for rel in file['relations'] if rel[1] == 'Hyponym']\n",
    "        \n",
    "        # create a 'stair' of combinations\n",
    "        \n",
    "        for i in range(len(entities)):\n",
    "            for j in range(i+1, len(entities)):\n",
    "                \n",
    "                #Extrac X and annotation names\n",
    "                ann_names = (entities[i][0],entities[j][0])\n",
    "                ann_names_reverted = (entities[j][0],entities[i][0])\n",
    "                \n",
    "                xdata = inputPair(entities[i], entities[j], file)\n",
    "                \n",
    "                data_case.append( (entities[i][0]+'-'+entities[j][0]) )\n",
    "                \n",
    "                #Reformat X\n",
    "                if indices:\n",
    "                    xout = createX(xdata, vocab_w, vocab_dist, vocab_ent, vocab_pos)\n",
    "                    #xout createXEmbeddings(xdata, engbedder, vocab_dist, vocab_ent, vocab_pos)\n",
    "                    data_X.append(np.array(xout))\n",
    "                else:\n",
    "                    data_X.append(np.array(xdata))\n",
    "                \n",
    "                ### Extract label\n",
    "                if ann_names in labels['hyponyms']:\n",
    "                    data_Y.append('hyponym')\n",
    "                elif ann_names_reverted in labels['hyponyms']:\n",
    "                    data_Y.append('hyponym_reverted')\n",
    "                elif ann_names in labels['synonyms'] or ann_names_reverted in labels['synonyms']:\n",
    "                    data_Y.append('synonym')\n",
    "                else:\n",
    "                    data_Y.append('NONE')\n",
    "                    \n",
    "    data_Y_np = np.array(data_Y)\n",
    "    \n",
    "    print('X is a list of datapoints where datapoint as an np.array with shape (sentence_length, feature_length). These can\\'t be turned into 3d np.array because sentence length vary')\n",
    "    print('Length of Y: {}'.format(len(data_Y_np)))\n",
    "    print('The 4 possible labels with count:')\n",
    "    pp(Counter(data_Y_np).most_common())\n",
    "    \n",
    "    return data_case, data_X, data_Y_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generer data_X og data_Y (hvor data_X er i ord)\n",
    "data_case, data_X, data_Y = dataX_Y_format(dev_data,indices = False)\n",
    "\n",
    "if True:\n",
    "    print(data_case[0])\n",
    "    pp(data_X[0])\n",
    "    print(data_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generer data_X og data_Y (hvor data_X er med indices)\n",
    "data_case, data_X, data_Y = dataX_Y_format(dev_data,indices = True)\n",
    "\n",
    "if True:\n",
    "    print(data_case[0])\n",
    "    pp(data_X[0])\n",
    "    print(data_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can test if conversion and back-conversion works well \n",
    "dev_data = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"dev\"))\n",
    "save_to_ann(dev_data, join(_snlp_book_dir, \"data\", \"scienceie\", \"dev_copy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## <font color='blue'>Task 1.1</font>: Develop and Train a Relation Extraction Model with Gold Keyphrases\n",
    "\n",
    "In this task, you develop a relation extraction model and apply it to the ScienceIE dataset.\n",
    "As input to it, at test time, you will have the plain input texts as well as `.ann` files containing gold (i.e. correct) keyphrase annotations. The output should be `.ann` files containing relations between those keyphrases (you should include the keyphrase annotations in the output file as well).\n",
    "\n",
    "A test input/output example is given in folders `data/scienceie/test/`,`data/scienceie/test_pred/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no strict requirements for how to design this model. You are expected to use the knowledge you have gathered throughout this course to design and implement this model. \n",
    "\n",
    "You are welcome to re-use existing code you might have written for other assignments as you see fit.\n",
    "\n",
    "You are free to implement your solution in either PyTorch or Tensorflow, but if you are not sure where to start, we recommend looking at the [Keras API](https://keras.io) which is [integrated into Tensorflow 1.14.0](https://www.tensorflow.org/beta/guide/keras/overview?hl=en)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should improve this cell\n",
    "\n",
    "def create_model(train_data, dev_data):\n",
    "    \"\"\"\n",
    "    Return an instance of a relation extraction model defined over the dataset.\n",
    "    Args:\n",
    "        train_data: the training data the relation extraction detection model should be defined over.\n",
    "        dev_data: the development data the relation extraction detection model can be tuned on.\n",
    "    Returns:\n",
    "        a relation extraction model\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def train_model(model, train_data, dev_data):\n",
    "    \"\"\"Train a relation extraction model on the given dataset.\n",
    "    Args:\n",
    "        model: The model to train\n",
    "        data_train: The dataset to train on\n",
    "        dev_data: the development data the relation extraction detection model can be tuned on\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def make_predictions(model, data):\n",
    "    \"\"\"Makes predictions on a list of instances\n",
    "    Args:\n",
    "        model: The trained model\n",
    "        data: The dataset to evaluate on\n",
    "    Returns:\n",
    "        The model's predictions for the data.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to test on the dev set\n",
    "data_train = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"train\"))\n",
    "dir_dev = join(_snlp_book_dir, \"data\", \"scienceie\", \"dev\")\n",
    "data_dev = load_scienceie(datadir=dir_dev)\n",
    "\n",
    "model = create_model(data_train, [])\n",
    "train_model(model, data_train, [])\n",
    "\n",
    "data_pred = make_predictions(model, data_dev)\n",
    "dir_pred = join(_snlp_book_dir, \"data\", \"scienceie\", \"dev_pred\")\n",
    "save_to_ann(data_pred, dir_pred)\n",
    "\n",
    "calculateMeasures(dir_dev, dir_pred, \"keys\") # this will only evaluate the correctness of relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL! It will evaluate your model on an unseen dataset!\n",
    "shutil.rmtree(join(_snlp_book_dir, \"data\", \"scienceie\", \"test_pred\")) # clean after previous\n",
    "\n",
    "data_train = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"train\"))\n",
    "data_dev = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"dev\"))\n",
    "\n",
    "model = create_model(data_train, data_dev)\n",
    "train_model(model, data_train, data_dev)\n",
    "\n",
    "data_test = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"test\"))\n",
    "data_pred = make_predictions(model, data_test)\n",
    "dir_pred = join(_snlp_book_dir, \"data\", \"scienceie\", \"test_pred\")\n",
    "save_to_ann(data_pred, dir_pred)\n",
    "\n",
    "dir_gold = join(_snlp_book_dir, \"data\", \"scienceie\", \"test_gold\")\n",
    "calculateMeasures(dir_gold, dir_pred, \"keys\") # this will only evaluate the correctness of relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 1.1</font>: Correctness of the implementation (20 pts)\n",
    "\n",
    "We assess if your code implements a correct relation extraction model (10 points):\n",
    "\n",
    "* 0-5 pts: the model does not run correctly or does not constitute a relation extraction model\n",
    "* 5-10 pts: the model correctly implements the requirements\n",
    "\n",
    "Additionally, we will assess how well your model performs on an unseen test set (10 points):\n",
    "\n",
    "* 0-5 pts: performance worse than a simple baseline model\n",
    "* 5-10 pts: performance better than a simple baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 1.2</font>: Describe your Approach\n",
    "\n",
    "Enter a maximum 500 words description of your model developed in Task 1.1, its architecture, and the way you trained and tuned it. Motivate your choices, describing potential benefits and downsides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 1.2</font>: Modelling Choices and Motivation (10 pts)\n",
    "\n",
    "\n",
    "Finally, we assess your modelling design choices and how you motivated them, which you summarised in the above cell (10 points):\n",
    "\n",
    "* 0-5 pts: the model design choices do not show high levels of creativity, e.g. re-using code from the lecture out of the box; and they are not moviated well\n",
    "* 5-10 pts: the model design choices show high levels of creativity, e.g. combining different things learned throughout the course, models inspired by further reading, etc.; and they are motivated well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## <font color='blue'>Task 2</font>: Relation Extraction with Weak Supervision\n",
    "\n",
    "In this task, the goal is to improve the performance of your model developed in Task 2 by obtaining more automatically labelled training data using a weak supervision approach. You are not required to change the relation extraction model architure, i.e. it is fine to re-use the one from Task 1, but instead, the requirements are to implement one or more weak supervision strategies.\n",
    "\n",
    "Some possible weak supervision methods for relation extraction will be introduced in the lecture Week 43 (https://github.com/copenlu/stat-nlp-book/blob/master/chapters/relation_extraction_slides.ipynb); the following blog post also serves as a good introduction to this topic: https://hazyresearch.github.io/snorkel/blog/ws_blog_post.html\n",
    "\n",
    "For this task, you are not confined to the training data provided to you, but you are welcome to obtain additional unlabelled datasets and automatically label them using weak supervision methods. \n",
    "\n",
    "The general setup will otherwise be the same as for Task 1:\n",
    "As input to it, you will have the plain input texts as well as `.ann` files containing gold (i.e. correct) keyphrase annotations. The output should be `.ann` files containing relations between those keyphrases.\n",
    "\n",
    "**Important notes**:\n",
    "- You must provide code for the functions below. \n",
    "- If running them on the full dataset exceeds the 10 minute limit, you are welcome to additionally provide a line of code that (down)loads the already weakly annotated data.\n",
    "- The maximum file size for weakly annotated data may not exceed 1GB.\n",
    "\n",
    "A test input/output example is given in folders `data/scienceie/test/`,`data/scienceie/test_pred/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should improve this cell\n",
    "\n",
    "def create_weak_model(train_data, dev_data, **args):\n",
    "    \"\"\"\n",
    "    Return an instance of a relation extraction model defined over the dataset.\n",
    "    Args:\n",
    "        train_data: the training data the relation extraction detection model should be defined over.\n",
    "        dev_data: the development data the relation extraction detection model can be tuned on.\n",
    "        **args: any additional arguments needed, e.g. additional automatically labelled training data\n",
    "    Returns:\n",
    "        a relation extraction model\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def train_weak_model(model, train_data, dev_data, **args):\n",
    "    \"\"\"Train a relation extraction model on the given dataset.\n",
    "    Args:\n",
    "        model: The model to train\n",
    "        data_train: The dataset to train on\n",
    "        dev_data: the development data the relation extraction detection model can be tuned on\n",
    "        **args: any additional arguments needed, e.g. additional automatically labelled training data\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def make_predictions_weak(model, data):\n",
    "    \"\"\"Makes predictions on a list of instances. Can be the same as function developed in Task 1.\n",
    "    Args:\n",
    "        model: The trained model\n",
    "        data: The dataset to evaluate on\n",
    "    Returns:\n",
    "        The model's predictions for the data.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a model and evaluating it on the development set. \n",
    "# Use this to monitor the performance of your model prior to submitting your assignment.\n",
    "data_train = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"train\"))\n",
    "dir_dev = join(_snlp_book_dir, \"data\", \"scienceie\", \"dev\")\n",
    "data_dev = load_scienceie(datadir=dir_dev)\n",
    "\n",
    "model = create_weak_model(data_train, [data_dev])\n",
    "train_weak_model(model, data_train, data_dev)\n",
    "\n",
    "data_pred = make_predictions_weak(model, data_dev)\n",
    "dir_pred = join(_snlp_book_dir, \"data\", \"scienceie\", \"dev_pred\")\n",
    "save_to_ann(data_pred, dir_pred)\n",
    "\n",
    "calculateMeasures(dir_dev, dir_pred, \"keys\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL! It will evaluate your model on an unseen dataset!\n",
    "shutil.rmtree(join(_snlp_book_dir, \"data\", \"scienceie\", \"test_pred\")) # clean after previous\n",
    "\n",
    "data_train = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"train\"))\n",
    "data_dev = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"dev\"))\n",
    "\n",
    "model = create_weak_model(data_train, data_dev)\n",
    "train_weak_model(model, data_train, data_dev)\n",
    "\n",
    "data_test = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"test\"))\n",
    "data_pred = make_predictions_weak(model, data_test)\n",
    "dir_pred = join(_snlp_book_dir, \"data\", \"scienceie\", \"test_pred\")\n",
    "save_to_ann(data_pred, dir_pred)\n",
    "\n",
    "dir_gold = join(_snlp_book_dir, \"data\", \"scienceie\", \"test_gold\")\n",
    "calculateMeasures(dir_gold, dir_pred, \"keys\") # this will only evaluate the correctness of relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_pred = join(_snlp_book_dir, \"data\", \"scienceie\", \"test_pred\")\n",
    "dir_gold = join(_snlp_book_dir, \"data\", \"scienceie\", \"test_gold\")\n",
    "\n",
    "dir_gold = join(_snlp_book_dir, \"data\", \"scienceie\", \"test_gold\")\n",
    "calculateMeasures(dir_gold, dir_pred, \"keys\") # this will only evaluate the correctness of relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 2.1</font>: Correctness of the implementation (20 pts)\n",
    "\n",
    "We assess if your code implements correct weak supervision methods (10 points):\n",
    "\n",
    "* 0-5 pts: the model does not run correctly or the methods implemented do not constitute weak supervision strategies\n",
    "* 5-10 pts: the model correctly implements the requirements\n",
    "\n",
    "Additionally, we will assess how well your model performs on an unseen test set (10 points):\n",
    "\n",
    "* 0-5 pts: performance worse than a simple baseline model\n",
    "* 5-10 pts: performance better than a simple baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 2.2</font>: Describe your Approach\n",
    "\n",
    "Enter a maximum 500 words description of your weak supervision stategies developed in Task 2.1 and the way you trained and tuned them. Motivate your choices, describing potential benefits and downsides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 2.2</font>: Modelling Choices and Motivation (10 pts)\n",
    "\n",
    "\n",
    "Finally, we assess your modelling design choices and how you motivated them, which you summarised in the above cell (10 points):\n",
    "\n",
    "* 0-5 pts: the model design choices do not show high levels of creativity, e.g. re-using code from the lecture out of the box; and they are not moviated well\n",
    "* 5-10 pts: the model design choices show high levels of creativity, e.g. combining different things learned throughout the course, models inspired by further reading, etc.; and they are motivated well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 3</font>: Comparison of relation extraction models\n",
    "\n",
    "Reflect on the models implemented in Tasks 1 and 2. What worked and didn't work well, and how would you explain this? How and when does the performance differ between the models and why might that be? You are expected to perform a small error analysis on the development set in order to answer these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 3</font>: Assess your explanation (20 pts)\n",
    "\n",
    "We will mark the explanation along the following dimension: \n",
    "\n",
    "* Substance (20pts: well-designed error analysis, correctly explained reasons for performance differences between models)"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
