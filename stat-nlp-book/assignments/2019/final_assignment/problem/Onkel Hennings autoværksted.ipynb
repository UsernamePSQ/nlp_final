{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! SETUP 1\n",
    "import sys, os\n",
    "_snlp_book_dir = \"../../../../\"\n",
    "sys.path.append(_snlp_book_dir) \n",
    "import math\n",
    "from glob import glob\n",
    "from os.path import isfile, join\n",
    "from statnlpbook.vocab import Vocab\n",
    "from statnlpbook.scienceie import calculateMeasures\n",
    "import shutil\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General modules\n",
    "import pprint\n",
    "from gensim.models import fasttext\n",
    "from gensim.models import KeyedVectors\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "### Our own modules\n",
    "from Extra_files.modules.load_and_save import load_scienceie, save_to_ann, reformat_to_save\n",
    "from Extra_files.modules.rules import _add_rules\n",
    "from Extra_files.modules.error_analysis import plot_confusion_matrix, get_dataframe\n",
    "from Extra_files.modules.DataPreparation import entityLocator, addPOStoDic, addLemmatoDic, inputPair\n",
    "from Extra_files.modules.MasterVocab import MasterVocab\n",
    "from Extra_files.modules.scaling import downscale\n",
    "from Extra_files.models.dummy_model import _sebastians_dummy_model\n",
    "from Extra_files.modules.Outputter import checkLegality, mostProbableYetLegal\n",
    "from Extra_files.modules.CalcMeasuresTMP import calcMeasures, normaliseAnn\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1,depth = None, compact = True).pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities removed due to overlap: 269 out of 1330\n",
      "Number of entities not identified in text: 1 out of 1061\n",
      "Number of relations lost due to overlap: 10 out of 168\n",
      "Removed references.\n",
      "Concatenated 'i.e.' and 'e.g.'.\n"
     ]
    }
   ],
   "source": [
    "from pickle import load, dump\n",
    "\n",
    "# data_m_XY = dataX_Y_format(train_data,indices = True) \n",
    "# data_m_XY_dev = dataX_Y_format(dev_data,indices = True)\n",
    "# dump(data_m_XY, open('Extra_files/resources/train_ind.pkl', 'wb'))\n",
    "\n",
    "data_m_XY = load(open('Extra_files/resources/train_ind.pkl', 'rb'))\n",
    "data_m_XY_dev = load(open('Extra_files/resources/dev_ind.pkl', 'rb'))\n",
    "dev_data = load_scienceie(datadir=join(_snlp_book_dir, \"data\", \"scienceie\", \"dev\"))\n",
    "if False:\n",
    "    print(data_m_XY['metadata'][0])\n",
    "    pp(data_m_XY['data_X'][0])\n",
    "    print(data_m_XY['data_Y'][0])\n",
    "    \n",
    "data = dev_data[list(dev_data.keys())[10]] # this one has many relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('*', 'Synonym', 'T4', 'T3'), ('*', 'Synonym', 'T6', 'T5'),\n",
      " ('R1', 'Hyponym', 'T15', 'T31'), ('R2', 'Hyponym', 'T16', 'T31'),\n",
      " ('R3', 'Hyponym', 'T17', 'T31'), ('R4', 'Hyponym', 'T18', 'T31'),\n",
      " ('R5', 'Hyponym', 'T19', 'T31')]\n"
     ]
    }
   ],
   "source": [
    "pp(data['relations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    predictions = np.array([[0.4, 0.7, 0.3, 0.1], [0.8, 0.2, 0.5, 0.1], [0.5, 0.3, 0.6, 0.2], [0.9, 0.1, 0.0, 0.0]])\n",
    "    #mostProbableYetLegal(predictions)\n",
    "\n",
    "    # rank by highest probability\n",
    "    probs = np.max(predictions, axis = 1)\n",
    "    highestorder = np.argsort(probs)[::-1] #flip the order\n",
    "    print(highestorder)\n",
    "    # sort predictions\n",
    "    pred_sorted = predictions[highestorder]\n",
    "    legalpreds = []\n",
    "    checkpreds = []\n",
    "    for pred in pred_sorted:\n",
    "        print(legalpreds)\n",
    "        checkpreds = deepcopy(legalpreds)\n",
    "        checkpreds.append(pred)\n",
    "        print(legalpreds)\n",
    "        # append if legal\n",
    "        if checkLegality(checkpreds):\n",
    "            legalpreds.append(pred)\n",
    "        #end-if\n",
    "    #end-for\n",
    "    print(legalpreds)\n",
    "\n",
    "    print(\"the function\")\n",
    "    mostProbableYetLegal(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    targetann = open(\"fakeANN/fakePred/example.ann\", \"r\")\n",
    "    pp(normaliseAnn(targetann, \"\"))\n",
    "    targetann.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.ann file missing in fakeANN/fakePred/. Assuming no predictions are available for this file.\n",
      "10.ann file missing in fakeANN/fakePred/. Assuming no predictions are available for this file.\n",
      "11.ann file missing in fakeANN/fakePred/. Assuming no predictions are available for this file.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'ent2' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1015a4a45ef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalcMeasures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fakeANN/fakeGold/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fakeANN/fakePred/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/nlp_final/stat-nlp-book/assignments/2019/final_assignment/problem/Extra_files/modules/CalcMeasuresTMP.py\u001b[0m in \u001b[0;36mcalcMeasures\u001b[0;34m(folder_gold, folder_pred, remove_anno)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mf_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mres_full_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspans_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrels_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormaliseAnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_anno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" file missing in \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfolder_pred\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\". Assuming no predictions are available for this file.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/nlp_final/stat-nlp-book/assignments/2019/final_assignment/problem/Extra_files/modules/CalcMeasuresTMP.py\u001b[0m in \u001b[0;36mnormaliseAnn\u001b[0;34m(file_anno, remove_anno)\u001b[0m\n\u001b[1;32m    161\u001b[0m                     \u001b[0ment2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ment1_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mspans_anno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ment1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mres_anno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr_g_offs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mrels_anno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr_g_offs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'ent2' referenced before assignment"
     ]
    }
   ],
   "source": [
    "calcMeasures(\"fakeANN/fakeGold/\", \"fakeANN/fakePred/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What works and when:**<br>\n",
    "[ E ] indicates error, [ C ] indicates that the case runs <br>\n",
    "1.ann : [ C ] Number of entities is below 2<br>\n",
    "2.ann : [ E ] Number of entities is below 2 and includes relations: **\"local variable 'ent2' referenced before assignment\"**<br>\n",
    "3.ann : [ E ] Number of entities is below 2 and relations reference only non-existing entities: **\"local variable 'ent1' referenced before assignment\"**<br>\n",
    "4.ann : [ E ] Relations reference non-existing entities:**\"local variable 'ent1' referenced before assignment\"**<br>\n",
    "5.ann : [ C ] Relation between the same entity: Hyponym-of Arg1:T1 Arg2:T1<br>\n",
    "6.ann : [ C ] Relation between the same entity: Synonym-of T1 T1<br>\n",
    "7.ann : [ C ] No ANN file/misnamed (shouldn't happen but sure)<br>\n",
    "8.ann : [ C ] Circular relations: Hyponym-of Arg1:T5 Arg2:T6 & Hyponym-of Arg1:T6 Arg2:T5 <br>\n",
    "9.ann : [ C ] Conflicting relation on same entities: Synonym-of and Hyponym-of T1 T2 <br>\n",
    "10.ann: [ C ] Duplicate relations <br>\n",
    "11.ann: [ E ] Relations reference non-existing entities, BUT has some valid relations in the beginning <br>\n",
    "12.ann: [ E ] Relations reference non-existing entities, BUT has only valid relations in the end (not in the beginning) <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
