{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Welcome to this interactive book on Statistical Natural Language Processing (NLP). NLP is a field that lies in the intersection of Computer Science, Artificial Intelligence (AI) and Linguistics with the goal to enable computers to solve tasks that require natural language _understanding_ and/or _generation_. Such tasks are omnipresent in most of our day-to-day life: think of [Machine Translation](https://www.bing.com/translator/), Automatic [Question Answering](https://www.youtube.com/watch?v=WFR3lOm_xhE) or even basic [Search](https://www.google.co.uk). All these tasks require the computer to process language in one way or another. But even if you ignore these practical applications, many people consider language to be at the heart of human intelligence, and this makes NLP (and it's more linguistically motivated cousin, [Computational Linguistics](http://en.wikipedia.org/wiki/Computational_linguistics)), important for its role in AI alone.              \n",
    "\n",
    "### Statistical NLP\n",
    "NLP is a vast field with beginnings dating back to at least the 1960s, and it is difficult to give a full account of every aspect of NLP. Hence, this book focuses on a sub-field of NLP termed Statistical NLP (SNLP). In SNLP computers aren't directly programmed to process language; instead, they _learn_ how language should be processed based on the _statistics_ of a corpus of natural language. For example, a statistical machine translation system's behaviour is affected by the statistics of a _parallel_ corpus where each document in one language is paired with its translation in another. This approach has been dominating NLP research for almost two decades now, and has seen widespread in industry too. Notice that while Statistics and Machine Learning are, in general, quite different fields, for the purposes of this book we will mostly identify Statistical NLP with Machine Learning-based NLP.\n",
    "\n",
    "### NDAK18000U Course Information\n",
    "We will use materials from this interactive book throughout the course. Note that this book was originally developed for a [15 ECTS course at UCL](https://github.com/uclmr/stat-nlp-book), so we will not be covering all topics of the book and will cover some in less depth. For completeness and context, you can still access all book materials below. \n",
    "Materials are due to minor changes. Materials covered in each week are listed below and will be linked once they are finalised. The course schedule is tentative and subject to minor changes.\n",
    "The official course description can be found [here](https://kurser.ku.dk/course/ndak18000u/2019-2020).\n",
    "\n",
    "  * Week 36\n",
    "      * Lecture (Tuesday): Course Logistics ([slides](chapters/course_logistics.ipynb)), Introduction to NLP ([slides](chapters/introduction.ipynb)), Tokenisation & Sentence Splitting ([notes](chapters/tokenization.ipynb), [slides](chapters/tokenization_slides.ipynb), [exercises](exercises/tokenization.ipynb)), Text Classification ([slides](chapters/doc_classify_slides_short.ipynb))\n",
    "      * Lab (Friday + Monday the following week): Jupyter notebook setup ([slides](labs/lab_1.ipynb))\n",
    "  * Week 37\n",
    "      * Reading (before lecture): [Jurafsky & Martin Chapter 7 up to and including 7.4](https://web.stanford.edu/~jurafsky/slp3/7.pdf)\n",
    "      * Lecture (Tuesday): Representation and Deep Learning ([slides](chapters/dl-representations.ipynb))\n",
    "      * Lab (Friday + Monday the following week)\n",
    "  * Week 38\n",
    "      * Reading (before lecture): [Jurafsky & Martin Chapter 9, up to and including 9.6](https://web.stanford.edu/~jurafsky/slp3/9.pdf)\n",
    "      * Lecture (Tuesday): Language Modelling ([slides](chapters/language_models_slides.ipynb)) and RNNs for Task-Specific Representation Learning in NLP ([slides](chapters/rnn_slides_ucph.ipynb))\n",
    "      * Lab (Friday + Monday the following week)\n",
    "  * Week 39\n",
    "      * Reading (before lecture): [Jurafsky & Martin Chapter 8, up to and including 8.3](https://web.stanford.edu/~jurafsky/slp3/8.pdf), plus [Chapter 17.1 on Named Entity Recognition](https://web.stanford.edu/~jurafsky/slp3/17.pdf)\n",
    "      * Lecture (Tuesday): Sequence Labelling ([slides](chapters/sequence_labeling_slides.ipynb))\n",
    "      * Lab (Friday + Monday the following week)\n",
    "  * Week 40\n",
    "      * Reading (before lecture): [Jurafsky & Martin Chapter 15](https://web.stanford.edu/~jurafsky/slp3/15.pdf)\n",
    "      * Lecture (Tuesday): Parsing ([slides](chapters/dependency_parsing_slides.ipynb))\n",
    "      * Lab (Friday + Monday the following week)\n",
    "  * Week 41\n",
    "      * Reading (before lecture): [Koehn \"Neural Machine Translation\", Section 13.5 (\"Neural Translation Models\")](https://arxiv.org/pdf/1709.07809.pdf)\n",
    "      * Lecture (Tuesday): Machine Translation ([slides](chapters/neural_mt_slides.ipynb))\n",
    "      * Lab (Friday + Monday the following week)\n",
    "  * Week 43\n",
    "      * Reading (before lecture): [Recommender Systems](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf)\n",
    "      * Lecture (Tuesday): Relation Extraction ([slides](chapters/relation_extraction_slides.ipynb))\n",
    "      * Lab (Friday + Monday the following week)\n",
    "  * Week 44\n",
    "      * Reading (before lecture): [The State of Transfer Learning in NLP](http://ruder.io/state-of-transfer-learning-in-nlp/), [An Overview of Multi-Task Learning in Deep Neural Networks](https://arxiv.org/pdf/1706.05098.pdf)\n",
    "      * Lecture (Tuesday): Recent Topics in NLP \n",
    "      * Lab (Friday)  \n",
    "\n",
    "### Structure of this Book\n",
    "We think that to understand and apply SNLP in practice one needs knowledge of the following:\n",
    "\n",
    "  * Tasks (e.g. Machine Translation, Syntactic Parsing)\n",
    "  * Methods & Frameworks (e.g. Discriminative Training, Linear Chain models, Representation Learning)\n",
    "  * Implementations (e.g. NLP data structures, efficient dynamic programming)\n",
    "   \n",
    "The book is somewhat structured around the task dimension. That is, we will explore different methods, frameworks and their implementations, usually in the context of specific NLP applications.  \n",
    "\n",
    "On a higher level the book is divided into *themes* that roughly correspond to learning paradigms within SNLP, and which follow a somewhat chronological order: we will start with generative learning, then discuss discriminative learning, then cover forms of weaker supervision to conclude with representation and deep learning. As an overarching theme we will use *structured prediction*, a formulation of machine learning that accounts for the fact that machine learning outputs are often not just classes, but structured objects such as sequences, trees or general graphs. This is a fitting approach, seeing as NLP tasks often require prediction of such structures.\n",
    "\n",
    "### Table Of Contents\n",
    "* Course Logistics: [slides](chapters/course_logistics.ipynb)\n",
    "* Introduction to NLP: [slides](chapters/introduction.ipynb)\n",
    "* Structured Prediction: [notes](chapters/structured_prediction.ipynb), [slides](chapters/structured_prediction_slides.ipynb), [exercises](exercises/structured_prediction.ipynb)\n",
    "* Tokenisation and Sentence Splitting: [notes](chapters/tokenization.ipynb), [slides](chapters/tokenization_slides.ipynb), [exercises](exercises/tokenization.ipynb)\n",
    "* Generative Learning:\n",
    "    * Language Models (MLE, smoothing): [notes](chapters/language_models.ipynb), [slides](chapters/language_models_slides.ipynb), [exercises](exercises/language_models.ipynb)\n",
    "        * Maximum Likelihood Estimation: [notes](chapters/mle.ipynb), [slides](chapters/mle_slides.ipynb)\n",
    "    * Machine Translation (EM algorithm, beam-search): [notes](chapters/word_mt.ipynb), [slides](chapters/word_mt_slides.ipynb), [exercises](exercises/mt.ipynb)\n",
    "    * Constituent Parsing (PCFG, dynamic programming): [notes](chapters/parsing.ipynb), [slides](chapters/parsing_slides.ipynb), [exercises](exercises/parsing.ipynb)\n",
    "    * Dependency Parsing (transition based parsing): [notes](chapters/transition-based_dependency_parsing.ipynb), [slides](chapters/transition_slides.ipynb)\n",
    "* Discriminative Learning:\n",
    "    * Text Classification (logistic regression): [notes](chapters/doc_classify.ipynb), [slides1](chapters/doc_classify_slides.ipynb), [slides2](chapters/doc_classify_slides_short.ipynb)\n",
    "    * Sequence Labelling (linear chain models): [notes](chapters/sequence_labeling.ipynb), [slides](chapters/sequence_labeling_slides.ipynb)\n",
    "    * Sequence Labelling (CRF): [slides](chapters/sequence_labeling_crf_slides.ipynb)\n",
    "* Weak Supervision:\n",
    "    * Relation Extraction (distant supervision, semi-supervised learning) [notes](chapters/relation_extraction.ipynb), [slides](https://www.dropbox.com/s/xqq1nwgw1i0gowr/relation-extraction.pdf?dl=0), [interactive-slides](chapters/relation_extraction_slides.ipynb)\n",
    "* Representation and Deep Learning\n",
    "    * Overview and Multi-layer Perceptrons [slides](chapters/dl.ipynb)\n",
    "    * Word Representations [slides](chapters/dl-representations.ipynb)\n",
    "    * Textual Entailment (RNNs) [slides](chapters/dl_applications.ipynb)\n",
    "    * Recurrent Neural Networks [slides1](chapters/rnn_slides.ipynb), [slides2](chapters/rnn_slides_ucph.ipynb)\n",
    "\n",
    "#### Methods\n",
    "We have a few dedicated method chapters:\n",
    "\n",
    "* Structured Prediction: [notes](chapters/structured_prediction.ipynb)\n",
    "* Maximum Likelihood Estimation: [notes](chapters/mle.ipynb)\n",
    "* EM-Algorithm: [notes](chapters/em.ipynb)\n",
    "\n",
    "### Interaction\n",
    "\n",
    "The best way to learn language processing with computers is \n",
    "to process language with computers. For this reason this book features interactive \n",
    "code blocks that we use to show NLP in practice, and that you can use \n",
    "to test and investigate methods and language. We use the [Python language](https://www.python.org/) throughout this book because it offers a large number of relevant libraries and it is easy to learn.\n",
    "\n",
    "### Installation\n",
    "To install the book locally and use it interactively follow the installation instruction on [GitHub](https://github.com/uclmr/stat-nlp-book).\n",
    "\n",
    "\n",
    "### Tutorials\n",
    "\n",
    "Labs:\n",
    "* [Lab 1](labs/lab_1.ipynb)\n",
    "* [Lab 2](labs/lab_2.ipynb)\n",
    "* [Lab 3](labs/lab_3.ipynb)\n",
    "* [Lab 4](labs/lab_4.ipynb)\n",
    "* [Lab 5](labs/lab_5.ipynb)\n",
    "\n",
    "Setup tutorials:\n",
    "* [Azure tutorial](tutorials/azure_tutorial.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
